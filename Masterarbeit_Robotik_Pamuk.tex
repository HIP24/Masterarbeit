%
% FH Technikum Wien
% !TEX encoding = UTF-8 Unicode
%
% Erstellung von Master- und Bachelorarbeiten an der FH Technikum Wien mit Hilfe von LaTeX und der Klasse TWBOOK
%
% Um ein eigenes Dokument zu erstellen, müssen Sie folgendes ergänzen:
% 1) Mit \documentclass[..] einstellen: Master- oder Bachelorarbeit, Studiengang und Sprache
% 2) Mit \newcommand{\FHTWCitationType}.. Zitierstandard festlegen (wird in der Regel vom Studiengang vorgegeben - bitte erfragen)
% 3) Deckblatt, Kurzfassung, etc. ausfüllen
% 4) und die Arbeit schreiben (die verwendeten Literaturquellen in Literature.bib eintragen)
%
% Getestet mit TeXstudio mit Zeichenkodierung ISO-8859-1 (=ansinew/latin1) und MikTex unter Windows
% Zu beachten ist, dass die Kodierung der Datei mit der Kodierung des paketes inputenc zusammen passt!
% Die Kodierung der Datei twbook.cls MUSS ANSI betragen!
% Bei der Verwendung von UTF8 muss dnicht nur die Kodierung des Dokuments auf UTF8 gestellt sein, sondern auch die des BibTex-Files!
%
% Bugreports und Feedback bitte per E-Mail an latex@technikum-wien.at
%
% Versionen
% *) V0.7: 9.1.2015, RO: Modeline angepasst und verschoben
% *) V0.6: 10.10.2014, RO: Weitere Anpassung an die UK
% *) V0.5: 8.8.2014, WK: Literaturquellen überarbeitet und angepasst
% *) V0.4: 4.8.2014, WK: Initalversion in SVN eingespielt
%
\documentclass[MMR,Master,english]{style/twbook}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%
% Hier biblatex & Biber konfigurieren; Vergessen Sie nicht, dass Sie biber verwenden müssen um eine Bibliothek zu erzeugen
%
\usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
\addbibresource{literature/Literature.bib}
%
% Bei Bedarf bitte hier die Syntax-Highlightings anpassen
%
% own 
\usepackage{hyperref}
\usepackage{float}
\usepackage{comment}
\usepackage{csquotes}
\usepackage{pgfplots}
\usepackage{soul}
\usepackage{color}
\usepackage{xcolor}  % Required for custom colors
\sethlcolor{blue}
\setcounter{secnumdepth}{3} % in text 
\setcounter{tocdepth}{3} % contents
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{rotating}
\usepackage{tikz}
\usepackage{setspace}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}

\hypersetup{
    colorlinks=false, % Aktiviert farbige Links
    linkcolor={rgb,255:red,0;green,0;blue,255}, % Interne Links (z.B. Sections)
    citecolor=orange, % Zitate
    urlcolor={rgb,255:red,128;green,0;blue,128} % URLs
}

%
% Platzierungsoptionen 
%
\usepackage[final]{listings}
\lstset{captionpos=b, numberbychapter=false,caption=\lstname,frame=single, numbers=left, stepnumber=1, numbersep=2pt, xleftmargin=15pt, framexleftmargin=15pt, numberstyle=\tiny, tabsize=3, columns=fixed, basicstyle={\fontfamily{pcr}\selectfont\footnotesize}, keywordstyle=\bfseries, commentstyle={\color[gray]{0.33}\itshape}, stringstyle=\color[gray]{0.25}, breaklines, breakatwhitespace, breakautoindent}
\lstloadlanguages{[ANSI]C, C++, python, [gnu]make, gnuplot, Matlab}

%Formatieren des Quellcodeverzeichnisses
\makeatletter
% Setzen der Bezeichnungen für das Quellcodeverzeichnis/Abkürzungsverzeichnis in Abhängigkeit von der eingestellten Sprache
\providecommand\listacroname{}
\@ifclasswith{twbook}{english}
{%
    \renewcommand\lstlistingname{Code}
    \renewcommand\lstlistlistingname{List of Codes}
    \renewcommand\listacroname{List of Abbreviations}
}{%
    \renewcommand\lstlistingname{Quellcode}
    \renewcommand\lstlistlistingname{Quellcodeverzeichnis}
    \renewcommand\listacroname{Abkürzungsverzeichnis}
}
% Wenn die Option listof=entryprefix gewählt wurde, Definition des Entyprefixes für das Quellcodeverzeichnis. Definition des Macros listoflolentryname analog zu listoflofentryname und listoflotentryname der KOMA-Klasse
\@ifclasswith{scrbook}{listof=entryprefix}
{%
    \newcommand\listoflolentryname\lstlistingname
}{%
}
\makeatother
\newcommand{\listofcode}{\phantomsection\lstlistoflistings}

% Die nachfolgenden Pakete stellen sonst nicht benötigte Features zur Verfügung
\usepackage{blindtext}
%
% Einträge für Deckblatt, Kurzfassung, etc.
%
\title{Virtualization of a Real-Time Operating System for Robot Control with a Focus on Real-Time Compliance}
\author{Halil Pamuk, BSc}
\studentnumber{51842568}
%\author{Titel Vorname Name, Titel\and{}Titel Vorname Name, Titel}
%\studentnumber{XXXXXXXXXXXXXXX\and{}XXXXXXXXXXXXXXX}
\supervisor{Sebastian Rauh, MSc. BEng}
%\supervisor[Begutachter]{Titel Vorname Name, Titel}
%\supervisor[Begutachterin]{Titel Vorname Name, Titel}
%\secondsupervisor{Titel Vorname Name, Titel}
%\secondsupervisor[Begutachter]{Titel Vorname Name, Titel}
%\secondsupervisor[Begutachterinnen]{Titel Vorname Name, Titel}
\place{Wien}
\kurzfassung{Virtualisierung von Echtzeitbetriebssystemen für die Robotersteuerung bietet viele Vorteile im Vergleich zu herkömmlichen hardwarebasierten Ansätzen. Lösungen, die auf reiner Hardware basieren, sind oft für einen bestimmten Zweck gebaut und daher in ihrer Flexibilität eingeschränkt. Es ist schwierig, die Hardware-Topologie an neue Anforderungen anzupassen, und es kann kostspielig sein, insbesondere bei der Skalierung von Operationen. Der physische Zugang für Updates und Wartung ist herausfordernd, was zu Ausfallzeiten und Produktivitätsverlusten führt. Während die Virtualisierung diese Probleme adressiert, führt sie zu erhöhtem Overhead und Latenz.

\bigskip \noindent Diese Arbeit untersucht die Virtualisierung des proprietären Salamander 4-Betriebssystems unter Verwendung von QEMU/KVM. Salamander 4 wird mit Yocto gebaut und verwendet harte Echtzeit mit Xenomai 3. Das Hauptziel besteht darin, die Latenzlücke zwischen der Bare-Metal und der Virtualisierung zu überbrücken, um ein deterministisches und zuverlässiges Verhalten zu gewährleisten, das für Echtzeit-Roboteranwendungen entscheidend ist.
	
\bigskip \noindent Erste Latenzmessungen zeigten eine signifikante Leistungslücke zwischen der Bare-Metal und dem virtualisierten Setup. Daher wird ein umfangreicher Tuning-Prozess durchgeführt, um Echtzeit-Performance und Determinismus zu erreichen. Diese Modifikationen umfassen Konfigurationen, die das BIOS, den Kernel, das Host-Betriebssystem, die QEMU/KVM-Virtualisierungsschicht und das Salamander 4 Gast-Betriebssystem selbst betreffen. Die Worst-Case-Latenz wurde von 707.622~µs auf 17.134~µs reduziert, was der Bare-Metal-Performance von 10.709~µs nahekommt. Darüber hinaus wird die Verbesserung der Echtzeit-Performance und des Determinismus anhand einer Roboteranwendung validiert, bei der die abgestimmte Virtualisierung mit der nicht abgestimmten und der Bare-Metal-Version verglichen wird.
	
\bigskip \noindent Insgesamt bietet diese Arbeit eine umfassende Methodik, um ein virtualisiertes Gastsystem in einem Hostsystem mit deterministischem Verhalten echtzeitfähig zu machen.}
\schlagworte{Virtualisierung, Echtzeitsysteme, Latenzreduktion, Robotersteuerung}

\outline{Virtualization of Real-Time Operating Systems (RTOS) for robotic control has many advantages in comparison to traditional hardware-based approaches. Solutions based on pure hardware are often built for a distinct purpose and are hence limited in flexibility. It is difficult to adapt the hardware topology to new requirements and can be costly, especially when scaling operations. Physical access for updates and maintenance is challenging, leading to downtime and lost productivity. While virtualization addresses these issues, it introduces increased overhead and latency. 
	
\bigskip \noindent This thesis investigates the virtualization of the proprietary Salamander 4 operating system, using QEMU/KVM. Salamander 4 is built with Yocto and employs hard real-time with Xenomai 3. The primary objective is to bridge the latency gap between the virtualized and bare metal versions in order to ensure deterministic and reliable behavior, which is crucial for real-time robotic applications.

\bigskip \noindent Initial latency measurements revealed a significant performance gap between the bare metal and virtualized setup. Thus, an extensive tuning process is carried out to achieve real-time performance and determinism. These modifications involve configurations spanning the BIOS, kernel, host operating system, QEMU/KVM virtualization layer, and the Salamander 4 guest operating system itself. The worst-case latency was brought down from 707.622~µs to 17.134~µs, closely aligning with the bare metal performance of 10.709~µs. In addition, the improvement in real-time performance and determinism is validated using a robotic application, where the tuned virtualization is compared with the untuned and the bare metal version.

\bigskip \noindent Altogether, this thesis provides a comprehensive blueprint for making a virtualized guest system real-time capable in a host system with deterministic behavior.}
\keywords{Virtualization, Real-Time Systems, Latency Reduction, Robot Control}

\acknowledgements{First, I would like to thank my supervisor, Sebastian Felix Rauh MSc. BEng., for his expert guidance and assistance throughout this master's thesis. His insightful answers to all my questions and his calming presence helped me stay focused and composed.

\bigskip \noindent I want to thank my wife, Nour Pamuk, for her unwavering support, love, and patience. She gave me the strength to overcome this challenge. This work is as much hers as it is mine.

\bigskip \noindent Lastly, I am deeply grateful to my parents, Semra Pamuk and Mehmet Pamuk, and to my close family for their understanding and encouragement during my studies. Without their unconditional love and support, this work would not have been possible.}

\begin{document}

\maketitle
%
% .. und hier beginnt die eigentliche Arbeit. Viel Erfolg beim Verfassen!
%
%Die folgende Gliederung von Bachelor- und Masterarbeiten ist für die Robotik-Studiengänge
%verpflichtend vorgeschrieben.
%
%- Deckblatt
%- Eidesstattliche Erklärung (digital signiert)
%- Kurzfassung und Abstract mit Schlüsselwörtern/Keywords
%- Danksagung (optional)
%- Inhaltsverzeichnis

%- Einleitung (die genannten Punkte müssen keine eigenen Unterkapitel sein, müssen aber aus der Einleitung klar hervorkommen):
%   - Stand der Technik
%   - Problem- und Aufgabenstellung
%   - Zielsetzung
%- Hauptteil -> Gliederung je nach Thema
%   - Methodik
%   - Resultate
%   - Wirtschaftliche Betrachtung (optional)
%   - Diskussion
%- Zusammenfassung und Ausblick

%- Literaturverzeichnis
%- Abbildungsverzeichnis
%- Tabellenverzeichnis
%- Abkürzungsverzeichnis (optional)
%- Glossar (optional)
%- Anhang (optional)

\chapter{Introduction}\label{cha:introduction}
In today's industrial production and automation, robot systems are well established and of crucial importance. Robots must react to their environment and perform time-critical tasks within strict time constraints. Delays or errors can have catastrophic consequences in some cases. Modern operating systems nowadays are categorized into two types:  General-Purpose Operating System (GPOS) and Real-Time Operating System (RTOS). The key difference is whether the system is time-critical or not~\cite{canbazPerformanceAnalysisRealtime2022}. In a traditional GPOS, such as Windows or Linux-based distributions, a high-priority task cannot interrupt a kernel operation, as illustrated in Figure~\ref{fig:kernel_generic}. This makes them not suitable for real-time requirements, as they cannot guarantee deterministic execution times.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\columnwidth]{img/introduction/kernel_generic.png}
	\caption[Non-preemptible Kernel]{Non-preemptible Kernel~\cite{WhatRealtimeLinux}}
	\label{fig:kernel_generic}
\end{figure}

\noindent However, in an RTOS, a high-priority process can interrupt a lower-priority task, even if it is in the middle of a kernel operation, as shown in Figure~\ref{fig:kernel_rt}. An RTOS is specifically designed to react to events within fixed time limits and prioritize the execution of high-priority processes.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\columnwidth]{img/introduction/kernel_rt.png}
	\caption[Preemptible Kernel]{Preemptible Kernel~\cite{WhatRealtimeLinux}}
	\label{fig:kernel_rt}
\end{figure}

\noindent Many Linux distributions can function as both GPOS and RTOS with kernel modifications.

\section{Real-Time Operating Systems}
\noindent The RTOS structure is visualized in Figure~\ref{fig:rtos_structure}. The core component of an RTOS that enables real-time capabilities is the kernel. It is responsible for managing system resources, scheduling tasks, and ensuring deterministic behavior~\cite{malallahComprehensiveStudyKernel2021}. It employs preemptive scheduling mechanisms to allow high-priority tasks to preempt lower-priority tasks, ensuring that time-critical tasks are not delayed. Additionally, RTOS kernels are designed to allocate and manage memory resources efficiently and minimize interrupt latency, which is crucial for real-time applications that require immediate response to external events~\cite{wangRealtimeEmbeddedSystems2017}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.50\columnwidth]{img/introduction/rtos_structure.png}
	\caption[RTOS Structure]{RTOS Structure~\cite{wangRealtimeEmbeddedSystems2017}}
	\label{fig:rtos_structure}
\end{figure}

\noindent In these real-time operating systems, task scheduling is based on so-called priority-based preemptive scheduling~\cite{buttazzoHardRealtimeComputing2024}. Each task in a software application is assigned a priority. A higher priority means that a faster response is required. Preemptive task scheduling ensures a very fast response. Preemptive means that the scheduler can stop a currently running task at any point if it recognizes that another task needs to be executed immediately. The basic rule on which priority-based preemptive scheduling is based is that the task with the highest priority that is ready to run is always the task that must be executed. So, if both a task with a lower priority and a task with a higher priority are ready to run, the scheduler ensures that the task with the higher priority runs first. The lower-priority task is only executed once the higher priority task has been processed.

\bigskip \noindent Real-time systems are usually classified as either soft or hard real-time systems~\cite{lipariRealTimeSchedulingHard2015}. The difference lies exclusively in the consequences of a violation of the time limits~\cite{amarpreetHardRealTime2009}. Hard real-time is when the system stops operating if a deadline is missed, which can have catastrophic consequences. Soft real-time exists when a system continues to function even if it cannot perform the tasks within a specified time. If the system has missed the deadline, this has no critical consequences. The system continues to run, although it does so with undesirably lower output quality~\cite{queirozTestingLimitsGeneralpurpose2023}.

\section{Application Context}\label{sec:application_context}
This section briefly defines the context and scope within which this master's thesis was composed, highlighting the specific contributions and requirements set by SIGMATEK GmbH \& Co KG~\cite{pixelartSIGMATEKKompletteAutomatisierungssysteme}.

\begin{itemize}
	\item This work was written at SIGMATEK GmbH \& Co KG, a company that researches, develops, produces, and distributes automation technology for industrial machinery and plant engineering. SIGMATEK uses its own custom Linux-based operating system, to be run on their self-manufactured CPUs. This operating system will be referred to as ``Salamander 4'' in this work. The details of Salamander 4 are explained in Section~\ref{sec:guest_operating_system}.
	\item Salamander 4 is virtualized through Quick Emulator (QEMU), an open-source emulator and virtualizer that allows running different operating systems on a computer. QEMU is described in Section~\ref{sec:qemu}.
	\item Salamander 4 was created with Yocto, an open-source project that helps develop custom Linux-based systems. Yocto is discussed in Section~\ref{sec:yocto}.
	\item Salamander 4 employs hard real-time with Xenomai 3, a real-time framework for Linux that enables deterministic and low-latency performance. Xenomai 3 is detailed in Section~\ref{sec:xenomai}.
	\item The goal is to virtualize Salamander 4 and approach the performance of bare metal (hardware) through real-time performance tunings, focusing on latency requirements set by SIGMATEK. These modifications are the subject of Section~\ref{sec:real-time_tunings}.
	\item The \texttt{latency} tool of the Xenomai suite to measure the scheduling latency of the real-time OS was suggested to be used by SIGMATEK. The goal was set to achieve latency values below 50 µs in the duration of the measurement.
	\item SIGMATEK uses the VARAN bus for communication between machines and systems, a real-time Ethernet network that was specially developed for industrial automation. Its operating principle is outlined in Section~\ref{sec:varan}.
	\item For the purpose of replacing the physical CPU, SIGMATEK provided a PCI Insert Card module PCV 522, that was plugged into the PC, as explained in Subsection~\ref{subsec:setup_experiment_virtualized}. This component could then be used to communicate with the Input/Output (I/O) peripherals, just like the physical CPU could when the I/O modules were attached to it. Subsequently, the PCV 522 module of SIGMATEK was connected with the Pulse Width Module PW 022 of SIGMATEK over the VARAN bus and the Varan Connection Module VI 021 of SIGMATEK.
	\item The robotic application from Section~\ref{sec:robotic_application} was written in Lasal Class 2, an object-oriented programming tool of SIGMATEK and part of SIGMATEK's Engineering Platform LASAL, with client-server technology and graphic representation.
\end{itemize}

\section{Related Work and State of the Art}
This master's thesis builds upon a number of previous studies in the field of virtualization of real-time systems and the inherent latency. In this section, the most influential studies are presented and their relevance to the work is addressed.

\bigskip \noindent \citeauthor{perneelRealtimeCapabilitiesStandard2015}~\cite{perneelRealtimeCapabilitiesStandard2015} mention in their work that Linux was initially developed as a general-purpose operating system without real-time applications in mind. They talk about the evolution of real-time behavior in the standard Linux kernel and state that it has recently gained popularity among the real-time community, largely due to its open-source nature and stability. The authors explain how to transform Linux into a real-time operating system by implementing the PREEMPT-RT patch. The paper also emphasizes some kernel configurations that ensure that the system's real-time behavior is preserved during runtime. Even though these modifications to the kernel enable soft real-time performance, the authors underline the fundamental rule in real-time software, that latency improvements have a negative impact on throughput and kernel performance. This is due to the added overhead of the \texttt{CONFIG\_PREEMPT\_RT} option. They concluded with their Linux build and a testing application that Linux can be a viable option for real-time applications, if it is correctly configured. \citeauthor{reghenzaniRealTimeLinuxKernel2020}~\cite{reghenzaniRealTimeLinuxKernel2020} provide an extensive overview of the real-time Linux kernel research, specifically on the PREEMPT RT patch. \citeauthor{adamRealTimePerformanceResponse2021}~\cite{adamRealTimePerformanceResponse2021} applied the PREEMPT\_RT patch on both Raspberry Pi 3 and BeagleBone Black and achieved significantly lower latencies compared to standard Linux kernels.

\bigskip \noindent \citeauthor{cinqueVirtualizingMixedCriticalitySystems2022}~\cite{cinqueVirtualizingMixedCriticalitySystems2022} survey different virtualization approaches, including separation kernels, microkernels, enhanced general-purpose hypervisors, and lightweight virtualization solutions like containers and unikernels. Similarly, \citeauthor{sandstromVirtualizationTechnologiesEmbedded2013}~\cite{sandstromVirtualizationTechnologiesEmbedded2013}, and~\citeauthor{taccariEmbeddedRealTimeVirtualization2014}~\cite{taccariEmbeddedRealTimeVirtualization2014} discuss and compare the current state-of-the-art in virtualization technologies with a focus on embedded real-time platforms. The latter emphasizes that research on real-time systems is focusing on virtualization and multicore scheduling and underlines that flexibility and cost are key metrics. Both works acknowledge the great benefits of virtualization, that is, reducing overall hardware costs, since all non-critical activities and real-time control tasks can run on the same hardware. In addition,~\citeauthor{javierperezHowRealTime2022}~\cite{javierperezHowRealTime2022} state that hardware-based Programmable Logic Controllers (PLCs) are costly to maintain and lack the flexibility needed for modern, resource-intensive applications like Machine Learning (ML) or Artificial Intelligence (AI).

\bigskip \noindent Despite the promising advantages of virtualization, one must be aware of the potential drawbacks of it, too.~\citeauthor{guStateoftheArtSurveyRealTime2012}~\cite{guStateoftheArtSurveyRealTime2012} highlight issues like lock-holder preemption and task-grain scheduling.~\citeauthor{garcia-vallsChallengesRealtimeVirtualization2014}~\cite{garcia-vallsChallengesRealtimeVirtualization2014} identify challenges in supporting real-time applications in the cloud, including resource management, quality of service guarantees, temporal and spatial isolation, and network performance.~\citeauthor{scordinoRealTimeVirtualizationIndustrial2020}~\cite{scordinoRealTimeVirtualizationIndustrial2020} address the challenge of interference on shared hardware resources, which can degrade real-time performance.

\bigskip \noindent \citeauthor{maPerformanceTuningKVMbased2013}~\cite{maPerformanceTuningKVMbased2013}~\cite{junzhangPerformanceAnalysisKVMBased2010} describe in their work how KVM impacts real-time performance, which has helped this work gain insight into the base overhead of virtualization. When a physical interrupt occurs while the guest RTOS is running, there are at least six stages that need to be traversed before the interrupt can be delivered to the guest RTOS. Once the interrupt is delivered to the guest RTOS, it incurs additional latency, called Interrupt Routine Time (IRT), which is the time between when an interrupt is recognized and the first instruction of the corresponding Interrupt Service Routine (ISR) of the guest OS is started. These stages are the primary sources of latencies caused by KVM, they are briefly described in Figure~\ref{fig:kvm_stages}.

\vspace{2em}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[node distance = 0.35cm, auto, % Adjusted node distance
			block/.style={
					rectangle,
					draw,
					fill=blue!20,
					text width=30em,
					rounded corners,
					align=center,
					inner sep=0.5em
				},
			line/.style={draw, -latex'}
		]
		% Place nodes
		\node [block] (init) {\scriptsize \textbf{Guest Code}\\  The guest OS is executing its normal operations.};
		\node [block, below=of init] (vmexit) {\scriptsize \textbf{VM Exit}\\  External interrupt triggers VM exit. Hardware stores interrupt info in \\\vspace{-0.5em} Virtual Machine Control Structure (VMCS).};
		\node [block, below=of vmexit] (aftervmexit) {\scriptsize \textbf{After VM Exit}\\  Control returns to kernel space. KVM retrieves interrupt info from VMCS,\\\vspace{-0.5em} marks it as pending for guest.};
		\node [block, below=of aftervmexit] (hostisr) {\scriptsize \textbf{Host ISR}\\  Host Linux handles the interrupt.};
		\node [block, below=of hostisr] (hostscheduling) {\scriptsize \textbf{Host Scheduling}\\  Host kernel invokes scheduler. Ideally, guest RTOS process is not preempted.};
		\node [block, below=of hostscheduling] (beforevmentry) {\scriptsize \textbf{Before VM Entry}\\  KVM's kernel module prepares for VM entry. Detects pending interrupt,\\\vspace{-0.5em} injects virtual interrupt into guest by writing to VMCS.};
		\node [block, below=of beforevmentry] (vmentry) {\scriptsize \textbf{VM Entry}\\  Hardware emulates conventional interrupt handling for guest based on VMCS info.};
		\node [block, below=of vmentry] (guestirt) {\scriptsize \textbf{Guest IRT}\\  Guest's interrupt handler is invoked to service the interrupt.};
		% Draw edges
		\path [line] (init) -- (vmexit);
		\path [line] (vmexit) -- (aftervmexit);
		\path [line] (aftervmexit) -- (hostisr);
		\path [line] (hostisr) -- (hostscheduling);
		\path [line] (hostscheduling) -- (beforevmentry);
		\path [line] (beforevmentry) -- (vmentry);
		\path [line] (vmentry) -- (guestirt);
	\end{tikzpicture}
	\caption[Flowchart of Operations in a virtualized Environment]{Flowchart of Operations in a virtualized Environment (Own Figure, inspired by~\citeauthor{maPerformanceTuningKVMbased2013})}
	\label{fig:kvm_stages}
\end{figure}
\vspace{1em}

\noindent The paper also details several performance tuning methods, including CPU shielding and prioritization, which have been applied in this work.

\clearpage

\bigskip \noindent \citeauthor{broskyShieldedProcessorsGuaranteeing2003}~\cite{broskyShieldedProcessorsGuaranteeing2003} report in their work about the shielded CPU model for enhancing real-time performance in symmetric multiprocessor systems. By dedicating one or more CPUs to high-priority processes and interrupts, the goal is to provide deterministic execution of real-time applications and interrupt responses. The authors mention that there is no user interface for setting process CPU affinity in the standard Linux, while there is one for setting interrupt CPU affinity via the \texttt{/proc/irq/*/smp\_affinity} files. These files were used in this master's thesis. According to the authors, the shielded CPU model is especially useful for tasks that require guaranteed interrupt response times, very fast interrupt responses, and high-frequency, deterministic execution.

\bigskip \noindent In the course of writing this thesis, the white paper "Real-Time Performance Tuning Best Practice Guidelines for KVM-Based Virtual Machines"~\cite{RealTimePerformanceTuning2022} by Intel has been instrumental. Given that the working computer used for this work is equipped with an Intel processor, the guidelines presented in this document were particularly relevant and valuable for optimizing real-time performance in KVM-based virtual machines. Section~\ref{sec:real-time_tunings} contains an extensive tuning process, which involves configurations spanning the BIOS, kernel, host OS, QEMU/KVM virtualization layer, and the Salamander 4 OS itself. The document concludes that the described tuning methods effectively improve real-time performance for KVM-based VMs, even under heavy workloads.

\bigskip \noindent \citeauthor{yoonRealTimePerformanceAnalysis2009}~\cite{yoonRealTimePerformanceAnalysis2009} use some of these real-time tunings in their research, including CPU shielding, memory locking, and spinning nanosleep, for controlling humanoid robots equipped with around 60 servo motors and sensors. They implement EtherCAT for real-time communication of distributed devices, whereas this thesis uses Varan. The authors highlight the importance of timely data processing in robots that collect data from their environment and respond accordingly through their sensors and actuators. The authors conclude with an experiment that their robotic system could achieve deterministic control of actuators and sensors even under heavy system load through the use of Linux with the real-time preemption patch. Their research has similarities to this master's thesis in terms of real-time tunings for robot control. However, this thesis also includes the virtualization aspect that is central.

\bigskip \noindent \citeauthor{kiszkaLinuxRealTimeHypervisor2009}~\cite{kiszkaLinuxRealTimeHypervisor2009} specifically focuses on Linux as a hypervisor and analyzes its real-time capabilities when using KVM and QEMU. Most importantly for this master's thesis, the author warns of starvation when the priorities of QEMU's threads are raised and lifted into a real-time scheduling class. Starvation is when a process does not get the resources it needs for a long time because the resources are being allocated to other processes. A way to reduce this risk is by not using more virtual CPUs (vCPUs) than there are actual processor cores. Moreover, Linux has a feature that can limit how much CPU time all real-time tasks can use in the host system, which was important information for the real-time tunings of this thesis later.

\clearpage

\bigskip \noindent \citeauthor{mckenneyRealTimeVs2008}~\cite{mckenneyRealTimeVs2008} addresses the overheads associated with real-time Linux when using KVM and QEMU. The author lists several sources of overhead, including memory locking to avoid page-fault latencies, increased overhead of locking and interrupts due to more-aggressive preemption, threaded interrupts that permit long-running interrupt handlers to be preempted by high-priority real-time processes, real-time task scheduling that requires global scheduling, high-resolution timers for tens-of-microseconds accuracy and precision, and preemptible RCU which has slightly higher read-side overhead than classic RCU. This information helped understand the trade-offs involved in real-time Linux systems.

\bigskip \noindent \citeauthor{deoliveiraDemystifyingRealTimeLinux2020}~\cite{deoliveiraDemystifyingRealTimeLinux2020} critique the traditional \texttt{cyclictest} tool used for measuring scheduling latency, highlighting its limitations due to its black-box approach and lack of theoretical grounding. This contributed to the decision not to use this tool to measure the scheduling latency in this work. Instead, the \texttt{latency} tool was used, as was determined in Section~\ref{sec:application_context}.

\section{Problem and Task Definition}
As stated previously, virtualization of real-time systems has many advantages in comparison to traditional hardware-based approaches. Solutions based on pure hardware are often built for a distinct purpose and are hence limited in flexibility. It is difficult to adapt the hardware topology to new requirements or new applications. Moreover, during periods when the demand is lower, expensive hardware may sit idle, leading to inefficient use of resources and a poor return on investment~\cite{gabrielResourceAwareParameterTuning2019}. Another crucial factor is, of course, the high cost factor associated with procuring and maintaining separate hardware for each real-time system~\cite{bhardwajVirtualizationCloudComputing2021}. Especially when organizations scale their operations, they have to add more physical machines, and the expenses rise rapidly~\cite{abbasiExploringOpenStackScalable2023}. Besides, one needs physical access to the hardware for any updates or maintenance, which is hard to achieve, especially for systems in remote or difficult-to-access locations. This can lead to system downtime and may not only disrupt the operations but can also result in lost productivity and potential revenue~\cite{mahfoudRealTimePredictiveMaintenanceBased2024}.

\bigskip \noindent Virtualization promises to overcome these challenges~\cite{bhardwajVirtualizationCloudComputing2021}~\cite{queirozContainerbasedVirtualizationRealtime2024}~\cite{cinqueEvaluatingVirtualizationFog2023}. However, virtualization also has its own challenges and pitfalls. The additional layers of abstraction can lead to increased overhead and latency~\cite{casiniLatencyAnalysisVirtualization2021}~\cite{zhangEvaluatingOptimizingVirtualization2010}. This is not desired and can be particularly problematic for real-time systems that require strict timing constraints. Making sure that tasks run predictably and on time in a virtualized environment can be challenging. The hypervisor and other virtual components can cause variations in how long tasks take to execute~\cite{garcia-vallsChallengesRealtimeVirtualization2014}. When there are multiple virtual machines (VMs) sharing the same physical resources, the VMs can affect each other's performance and consequently violate real-time constraints~\cite{queirozContainerbasedVirtualizationRealtime2024}.

\clearpage

\bigskip \noindent In this master's thesis, the task is the virtualization of a real-time operating system to control a robot, with a focus on compliance with real-time determinism. Chapter~\ref{cha:methodology} explains in detail the procedure, how this was tackled. As already established in Section~\ref{sec:application_context}, Salamander 4 is built with Yocto, employs hard real-time with Xenomai 3, and is virtualized through QEMU/KVM. The latency values are measured with the \texttt{latency} program of the Xenomai tool suite. The initial situation is described in Section~\ref{sec:starting_situation}. In a nutshell, there is a significant initial gap in latency between the virtualized Salamander 4 and the Salamander 4 on bare metal. For this reason, an extensive tuning process is carried out to achieve real-time performance and determinism. These modifications involve configurations spanning the BIOS, kernel, host OS, QEMU/KVM virtualization layer, and the Salamander 4 guest OS itself. The individual configurations are discussed in detail in Section~\ref{sec:real-time_tunings} and the modifications are justified with a clear explanation.

\section{Objective}
The goal is to bring the latency of the virtualized Salamander 4 closer to that of the hardware (bare metal), thereby providing a comprehensive blueprint for making a virtualized guest system in a host system real-time capable with deterministic behavior. Section~\ref{sec:starting_situation} outlines the initial situation of this master's thesis and serves as the foundation for the real-time performance tunings discussed in Section~\ref{sec:real-time_tunings}. The objective is to achieve latency values below 50 µs during the measurement period through real-time tunings, with no outliers whatsoever. To determine and validate the performance of the tunings, the \texttt{latency} tool is used, as stated in Section~\ref{sec:application_context}. On top of that, the aim is to use a robotic application in a practical scenario to demonstrate and validate that the tunings significantly reduce latency and improve determinism compared to the unmodified virtualization.

\clearpage

\chapter{Methodology}\label{cha:methodology}

This chapter describes in detail all the theoretical concepts and practical methods that contributed to achieving the objectives of this master's thesis.

\section{Host Operating System}\label{sec:host_operating_system}
The host operating system is the basis for the virtualization of the guest operating system. In the course of this work, it is continuously tuned to achieve real-time behavior and determinism. Table~\ref{tab:testbed_configuration} illustrates the specifications of the host.

\begin{table}[H]
	\centering
	\caption[Host Operating System Configuration]{Host Operating System Configuration}
	\label{tab:testbed_configuration}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|l|l|}
			\hline
			\textbf{CPU}            & Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz, 6 cores \\ \hline
			\textbf{Memory}         & 4 $\times$ 8GB DDR4-2666/2400 MHz, 32GB          \\ \hline
			\textbf{GPU}            & Intel UHD Graphics 630 GPU                       \\ \hline
			\textbf{Storage}        & 500GB NVMe SSD                                   \\ \hline
			\textbf{BIOS}           & Dell Version 1.29.0                              \\ \hline
			\textbf{OS}             & Ubuntu 22.04.4 LTS                               \\ \hline
			\textbf{Kernel Version} & 6.8.0-40-generic                                 \\ \hline
		\end{tabular}}
\end{table}

\noindent Figure~\ref{fig:lstopo} is the output of the~\texttt{lstopo} command and visualizes the hardware nodes of the system, including CPU cores, caches, memory, and I/O devices.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.55\columnwidth]{img/methodology/lstopo.png}
	\caption[Host Operating System Hardware Topology]{Host Operating System Hardware Topology}
	\label{fig:lstopo}
\end{figure}

\section{Guest Operating System}\label{sec:guest_operating_system}
This section briefly describes the guest operating system, Salamander 4.

\subsection{Structure}
\noindent Salamander 4 is the proprietary operating system of SIGMATEK. It is based on Linux version 5.15.94 and integrates Xenomai 3.2, a real-time development environment. Salamander 4 is a 64-bit system, which refers to the x86\_64 architecture. The real-time behaviour is achieved through the use of Symmetric Multi-Processing (SMP) and Preemptive Scheduling (PREEMPT). In addition, it uses IRQPIPE to process interrupts in a way that meets the real-time requirements of the system. The output of the command~\texttt{uname -a} can be observed in Code~\ref{output:uname_a_virt}.

\vspace{2em}
\begin{lstlisting}[name={Guest Operating System Information},label={output:uname_a_virt}]
root@sigmatek-core2:~# uname -a
Linux sigmatek-core2 5.15.94 #1 SMP PREEMPT IRQPIPE Tue Feb 14 18:18:05 UTC 2023 x86_64 GNU/Linux
\end{lstlisting}

\noindent Salamander 4 is powered by SIGMATEK's CP 841~\cite{CPUUnitsSIGMATEK} and is comprised of the following software modules:

\begin{itemize}
	\item \textbf{Operating system}: The operating system in a LASAL CPU manages the hardware and software resources of the system. It is provided in a completely PC-compatible manner, working with a standard PC BIOS.
	\item \textbf{Loader}: The loader is a part of the operating system that is responsible for loading programs from executables into memory, preparing them for execution and then running them.
	\item \textbf{Hardware classes}: Hardware classes in LASAL represent the different types of hardware components that can be controlled by the LASAL CPU. They help organize and manage the hardware components in a modular manner. The graphical hardware editor in LASAL allows for a accurate simulation of the actual hardware.
	\item \textbf{Applications}: Applications are developed using LASAL CLASS 2~\cite{EngineeringToolLASAL}, a solution for automation tasks that supports object-oriented programming and design in compliance with IEC 61131-3.
\end{itemize}

\clearpage

\noindent These modules and the interfaces (indicated by an arrow) between them are shown in Figure~\ref{fig:lasal_cpu}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\columnwidth]{img/methodology/Software-Struktur_einer_LASAL_CPU.png}
	\caption[Structure of Salamander 4 CPU]{Structure of Salamander 4 CPU~\cite{LASALOSSIGMATEK}}
	\label{fig:lasal_cpu}
\end{figure}

\subsection{Memory Management}
\noindent For the sake of completeness, Figure~\ref{fig:memory_management} displays the memory management of Salamander 4. LRT stands for Lasal Runtime and creates an execution environment where applications developed using LASAL Class 2 can run. It provides real-time functions, data types, and other constructs for real-time programming.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\columnwidth]{img/methodology/RAM_Memory_management.png}
	\caption[Memory Management of Salamander 4]{Memory Management of Salamander 4~\cite{LASALOSSIGMATEK}}
	\label{fig:memory_management}
\end{figure}

\clearpage

\section{QEMU}\label{sec:qemu}

Quick Emulator (QEMU)~\cite{QEMU} is an open-source emulator and virtualizer that allows running operating systems and programs for one machine on a different machine. It can emulate different hardware architectures and is used in this thesis to emulate Salamander 4. When QEMU is used together with Kernel-based Virtual Machine (KVM), it can provide near-native performance by running code directly on the host CPU. The QEMU script used to emulate Salamander 4 and the meaning of the options are described in detail in Subsection \ref{subsec:salamander4-virtualization}.

\section{Yocto}\label{sec:yocto}

Salamander 4 is built with Yocto~\cite{WelcomeYoctoProject}, an open-source project that helps develop custom Linux-based operating systems for embedded devices. It is not a Linux version itself but a framework to make custom versions. From the kernel settings to the software packages included, almost everything in the Linux distribution can be adjusted with Yocto, hence it fits custom needs. Yocto organizes different components of the system into layers, making code management and updates easier. It builds the operating system with BitBake and automates the compilation and assembly of software. Upon generating the necessary files, Yocto provides a QEMU folder with the following components shown in Code~\ref{lst:ls}.

\vspace{2em}
\begin{lstlisting}[name={Contents of QEMU Folder for Salamander 4},label={lst:ls}]
    sigma_ibo@localhost:~/build/tmp/deploy/qemu/salamander-image$ ls -1
    bzImage
    drive-c
    ovmf.code.qcow2
    qemu_def.sh
    salamander-image-sigmatek-core2.ext4
    stek-drive-c-image-sigmatek-core2.tar.gz
    vmlinux
    \end{lstlisting}

\noindent The following is a description of the components used for the virtualization of Salamander 4.

\begin{itemize}
	\item \textbf{bzImage}: Compressed Linux kernel image that is loaded by QEMU at system start. The short form ``bz`` stands for big-zipped.
	\item \textbf{drive-c}: Directory serving as C drive for QEMU, created and filled by \texttt{qemu\_def.sh} script.
	\item \textbf{ovmf.code.qcow2}: Firmware file for QEMU that enables UEFI boot process. ~\texttt{OVMF} stands for Open Virtual Machine Firmware,~\texttt{qcow2} is a format for disk image files used by QEMU, it stands for ``QEMU Copy On Write version 2''.
	\item \textbf{qemu\_def.sh}: Shell script that starts QEMU with required parameters to boot the Salamamder 4 OS. It is described later in more detail.
	\item \textbf{salamander-image-sigmatek-core2.ext4}: Disk image of the Salamander 4 OS. It uses the~\texttt{ext4} file system and serves as the root file system in the QEMU virtual machine, acting as the virtual hard drive.
	\item \textbf{stek-drive-c-image-sigmatek-core2.tar.gz}: Compressed tarball containing a pre-configured environment for the Salamander 4 OS. It is unpacked and sets up the~\texttt{drive-c/} directory with system and log files in the~\texttt{qemu\_def.sh} script.
	\item \textbf{vmlinux}: Uncompressed Linux kernel image, typically used for debugging.
\end{itemize}

\noindent The initial QEMU script after the custom Yocto build and the starting point for this work is shown in Code~\ref{script:qemu_def}. This script is used to start QEMU with required parameters to boot the Salamander 4 OS. It will be adjusted in Section~\ref{sec:real-time_tunings} in order to accompany real-time performance tunings.

\vspace{2em}
\begin{lstlisting}[name={QEMU Script for starting Salamander 4 Virtualization},label={script:qemu_def}]
	#!/bin/sh

	if  [ ! -d drive-c/ ]; then
			echo "Filling drive-c/"
			mkdir drive-c/
			tar -C drive-c/ -xf stek-drive-c-image-sigmatek-core2.tar.gz
	fi
		
	exec qemu-system-x86_64 -M pc,accel=kvm -kernel ./bzImage \
	-m 2048 -drive file=salamander-image-sigmatek-core2.ext4,format=raw,media=disk \
	-append "console=ttyS0 console=tty1 root=/dev/sda rw panic=1 sigmatek_lrt.QEMU=1 ip=dhcp rootfstype=ext4" \
	-net nic,model=e1000,netdev=e1000 -netdev bridge,id=e1000,br=nm-bridge \
	-fsdev local,security_model=none,id=fsdev0,path=drive-c -device virtio-9p-pci,id=fs0,fsdev=fsdev0,mount_tag=/mnt/drive-C \
	-device vhost-vsock-pci,guest-cid=3,id=vsock0 \
	-drive if=pflash,format=qcow2,file=ovmf.code.qcow2 \
	-no-reboot -nographic
\end{lstlisting}
\vspace{2em}

\noindent This script is run on a generic Ubuntu 22.04.4 system, as mentioned previously in Section~\ref{sec:host_operating_system}.

\clearpage

\section{Xenomai}\label{sec:xenomai}

Xenomai 3~\cite{XenomaiXenomai} is an open-source real-time framework that offers two paths to real-time performance. The first approach supplements the Linux kernel with a compact real-time core dubbed Cobalt, demonstrated in Figure~\ref{fig:cobalt}. Cobalt runs side-by-side with Linux, but it handles all time-critical activities like interrupt processing and real-time thread scheduling with higher priority than the regular kernel activities.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\columnwidth]{img/introduction/xenomai/x3-cobalt-interfaces.png}
	\caption[Xenomai 3 Cobalt Interfaces]{Xenomai 3 Cobalt Interfaces~\cite{OverviewXenomai}}
	\label{fig:cobalt}
\end{figure}

\noindent  The second approach, called Mercury and shown in Figure~\ref{fig:mercury}, relies on the real-time capabilities already present in the native Linux kernel. Often, applications require the PREEMPT-RT patch~\cite{RealtimePreempt_rt_versionsWiki} to augment the mainline kernel's real-time responsiveness and minimize jitter, but this is not mandatory and depends on the application's specific requirements for responsiveness and tolerance for occasional deadline misses.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\columnwidth]{img/introduction/xenomai/x3-mercury-interfaces.png}
	\caption[Xenomai 3 Mercury Interfaces]{Xenomai 3 Mercury Interfaces~\cite{OverviewXenomai}}
	\label{fig:mercury}
\end{figure}

\noindent Salamander 4 uses the Cobalt real-time core with the Dovetail extension, which allows the kernel to handle real-time tasks with low latency. A key tool of the Xenomai suite that is used in this work is the \texttt{latency} tool~\cite{LATENCY}. It benchmarks the timer latency, which is the time it takes for the kernel to respond to timer interrupts or task activations. The tool creates real-time tasks or interrupt handlers and measures the difference between the expected and actual execution times.

\section{Trace-cmd}\label{sec:trace-cmd}
\texttt{Trace-cmd}~\cite{Tracecmd} is the front-end tool of the \texttt{ftrace}~\cite{FtraceFunctionTracer} tool that is built inside the Linux kernel and used for tracing the Linux kernel. \texttt{Trace-cmd} can record detailed reports of kernel events such as interrupts, scheduler decisions, function calls, and custom events in real time. It is easier with this tool to enable and disable tracing, set filters, and view results. \texttt{Trace-cmd} helped debugging and identifying the reasons for latency in Salamander 4 to continuously improve its real-time performance and determinism. A very practical aspect of \texttt{trace-cmd} is that the guest can be traced from the host. For this purpose, the guest needs to be configured with \texttt{vsocks}, which are a virtual socket that enable direct communication between the host and guest. To enable vsockets and tracing, the guest kernel requires the following configurations, according to Linux developer and maintainer Steve Rostedt~\cite{rostedtTracecmdHostGuest}.

\vspace{2em}
\begin{lstlisting}[language=bash, caption={Kernel Flags for Vsocks and Tracing}, label={lst:kernel-config}]
	# Vsockets settings
	CONFIG_VSOCKETS=m
	CONFIG_VHOST_VSOCK=m
	CONFIG_VIRTIO_VSOCKETS=m
	CONFIG_VIRTIO_VSOCKETS_COMMON=m
	CONFIG_VSOCKETS_DIAG=m
	CONFIG_VSOCKETS_LOOPBACK=m
	# Tracing settings
	CONFIG_TRACING=y
	CONFIG_FTRACE=y
	CONFIG_FUNCTION_TRACER=y
	CONFIG_FUNCTION_GRAPH_TRACER=y
	CONFIG_DYNAMIC_FTRACE=y
	CONFIG_DYNAMIC_FTRACE_WITH_REGS=y
	CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS=y
	CONFIG_DYNAMIC_FTRACE_WITH_ARGS=y
	CONFIG_SCHED_TRACER=y
	CONFIG_FTRACE_SYSCALLS=y
	CONFIG_TRACER_SNAPSHOT=y
	CONFIG_KPROBE_EVENTS=y
	CONFIG_UPROBE_EVENTS=y
	CONFIG_BPF_EVENTS=y
	CONFIG_DYNAMIC_EVENTS=y
	CONFIG_PROBE_EVENTS=y
	CONFIG_SYNTH_EVENTS=y
	CONFIG_HIST_TRIGGERS=y
	\end{lstlisting}

\clearpage

\noindent After compiling the kernel with the additional settings above, the guest needs access to the vsocket. In QEMU, the line \texttt{-device vhost-vsock-pci,guest-cid=3,id=vsock0} was added. This command configures a virtual socket (\texttt{vsock}) device in QEMU, assigning the guest a unique Context Identifier (\texttt{cid}) of 3 for direct communication with the host. The CID can be chosen as desired. Once the guest is booted with \texttt{vhost} enabled, the \texttt{sudo trace-cmd agent} command can be run.  The agent now listens to connections on port 823 by default with the specified CID, 3 in this case. The host can trace itself and the guest from this moment going forward. For example, \texttt{sudo trace-cmd record -e all -A @3:823 --name Salamander4 -e all} records all events from the host and guest. \texttt{sudo trace-cmd record -e kvm:kvm\_entry -e kvm:kvm\_exit -A @3:823 --name Salamander4 -e all} traces only the \texttt{kvm\_entry} and \texttt{kvm\_exit} events of the host along with the exit reasons and all guest events. As a last example, \texttt{sudo trace-cmd record -e kvm -e sched -e irq -e -A @3:823 --name Salamander4 -e all} enables tracing of KVM, scheduling, and interrupt events on the host, for capturing the KVM events that manage the guest, scheduling activities, and interrupt handling respectively.

\section{Kernelshark}\label{sec:kernelshark}
Viewing the tracing data that was recorded with \texttt{trace-cmd} on a text-based file can be laborious. A better way of analyzing the data is done through \texttt{KernelShark}~\cite{KernelShark}, which is a graphical front-end tool. It visualizes the recorded kernel trace data in a readable way on an interactive timeline, which facilitates the process of identifying patterns and correlations between events. The latency issues can be analyzed by further filtering the displayed events according to processes, event types, or time ranges. After the recording, \texttt{trace-cmd} generates a host \texttt{trace.dat} file and a guest \texttt{trace-Salamander4.dat} file, which in this case can be loaded into \texttt{KernelShark} with the command \texttt{kernelshark trace.dat -a trace-Salamander4.dat}. Once both files are loaded, the trace data can be visually inspected. All the current CPUs for both the host and the guest can be turned off, and the KVM Combo plots menu can be selected to view the guest vCPUs plotted on top of the host threads that run them, as depicted in Figure~\ref{fig:kernelshark_combo}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/methodology/kernelshark_combo.png}
	\caption[Guest vCPUs plotted on top of Host Threads in KernelShark]{Guest vCPUs plotted on top of Host Threads in KernelShark}
	\label{fig:kernelshark_combo}
\end{figure}

\section{VARAN-Bus}\label{sec:varan}

VARAN is a real-time Ethernet bus designed for industrial automation, capable of connecting larger systems and machines with smaller components and sensors in hard real-time. This bus operates on the manager/client principle, where each network includes a manager and one or more subordinate clients. Multiple VARAN networks can be linked together into a single synchronized network using a higher-level VARAN manager. The jitter between these systems is kept below 100 ns. This synchronization between systems and their data exchange is illustrated in Figure~\ref{fig:multiple_varans}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\columnwidth]{img/methodology/multiple_varans.png}
	\caption[Synchronization between Systems and their Data Exchange]{Synchronization between Systems and their Data Exchange}
	\label{fig:multiple_varans}
\end{figure}

\section{Approach}\label{sec:approach}
The goal is to virtualize the Salamander 4 OS and bring its real-time performance closer to that of the bare metal version and guarantee deterministic and reliable behavior. For that purpose, the first step is to apply the PREEMPT-RT patch to the host system and subsequently tune the real-time performance through BIOS, kernel, host, QEMU/KVM, and guest configurations. These modifications are added sequentially and tested together. First, the BIOS is configured, followed by the kernel. The BIOS settings are not reverted when moving to the kernel configurations. Next, the host is configured, but the BIOS and kernel settings remain unchanged. The guest OS is already based on Xenomai and therefore, no additional modifications are necessary for the guest OS itself. Finally, QEMU/KVM settings are applied. This means that the configurations are not isolated but are applied and then tested as a whole. The key metric here is the latency of the system, and the real-time performance is evaluated using the \texttt{latency} tool of the Xenomai suite after each configuration step. At the end, the results are demonstrated altogether and a holistic comparison and discussion are carried out. In addition, the improvement in real-time performance and determinism is demonstrated using a robotic application, where the tuned virtualization is compared with the untuned and the bare metal version.

\clearpage

\chapter{Implementation}

\section{Initial Situation}\label{sec:starting_situation}
As a starting point, initial latency values of both the bare metal and virtualization versions were measured with the \texttt{latency} tool of the Xenomai tool suite. Salamander 4 bare metal refers to the custom operating system developed by SIGMATEK, running on their proprietary hardware. Salamander 4 virtualization refers to a virtual version of Salamander 4, achieved through QEMU/KVM. Subsections~\ref{subsec:salamander4-bare-metal} and~\ref{subsec:salamander4-virtualization} specify the details of the measurements for both setups. In the further course of this master's thesis, the aim is to bring the latency values of the virtualization closer to those of bare metal and guarantee deterministic and reliable behavior.

\subsection{Salamander 4 Bare Metal}\label{subsec:salamander4-bare-metal}
As a reference point, the \texttt{latency} program was executed on Salamander 4 bare metal for a duration of 10 minutes. The complete command used was~\texttt{latency -h -g gnuplot.txt -T 600}, which runs the latency measurement tool for 600 seconds and prints histograms of minimum, average, and maximum latencies in a Gnuplot-compatible format to the file~\texttt{gnuplot.txt}. Figure~\ref{fig:gnuplot_max_latency_hardware} depicts the distribution of latency over the course of said time.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/implementation/gnuplot_max_latency_hardware.png}
	\caption[Latency Distribution of Salamander 4 Bare Metal]{Latency Distribution of Salamander 4 Bare Metal}
	\label{fig:gnuplot_max_latency_hardware}
\end{figure}

\noindent The statistics obtained from this measurement are provided in Table~\ref{tab:latency_stats_hardware} and Table~\ref{tab:latency_overrun_msw_hardware}. The test was conducted with a sampling period of 100~µs, using a periodic user-mode task, and was assigned a priority of 99. If any sample's latency value is greater than 100~µs, it would be considered an overrun.

\begin{table}[H]
	\centering
	\caption{Latency Parameters of Bare Metal}
	\label{tab:latency_stats_hardware}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Param} & \textbf{Samples} & \textbf{Average (µs)} & \textbf{Std Dev (µs)} \\ \hline
		\textbf{min}   & 599              & 0.711                 & 0.454                 \\ \hline
		\textbf{avg}   & 5,999,988        & 1.019                 & 0.150                 \\ \hline
		\textbf{max}   & 599              & 3.528                 & 0.895                 \\ \hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption[Latency Statistics with Overrun Counts of Bare Metal]{Minimum, Average, and Maximum Latency with Overrun Counts of Bare Metal}
	\label{tab:latency_overrun_msw_hardware}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Overruns} \\ \hline
		0.613                 & 1.380                 & 10.709                & 0                 \\ \hline
	\end{tabular}
\end{table}

\subsection{Salamander 4 Virtualization}\label{subsec:salamander4-virtualization}
In addition to providing Salamander 4 on its own hardware, SIGMATEK has also developed a virtualized version of this operating system. Figure~\ref{fig:gnuplot_max_latency_default} shows the distribution of latency measured over 10 minutes using the default QEMU script in Code~\ref{script:qemu_def} without any further adjustments.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/implementation/gnuplot_max_latency_default.png}
	\caption[Latency Distribution of Salamander 4 Untuned Virtualization]{Latency Distribution of Salamander 4 Untuned Virtualization}
	\label{fig:gnuplot_max_latency_default}
\end{figure}

\clearpage

\noindent The statistics obtained from this measurement are provided in Table~\ref{tab:latency_stats_virt} and Table~\ref{tab:latency_overrun_msw_new_virt}. The test was again conducted with a sampling period of 100~µs, using a periodic user-mode task, and was assigned a priority of 99.

\begin{table}[H]
	\centering
	\caption{Latency Parameters of Untuned Virtualization}
	\label{tab:latency_stats_virt}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Param} & \textbf{Samples} & \textbf{Average (µs)} & \textbf{Std Dev (µs)} \\ \hline
		\textbf{min}   & 599              & 3.713                 & 1.355                 \\ \hline
		\textbf{avg}   & 5,999,922        & 8.247                 & 2.521                 \\ \hline
		\textbf{max}   & 599              & 45.705                & 52.196                \\ \hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption[Latency Statistics with Overrun Counts of Untuned Virtualization]{Minimum, Average, and Maximum Latency with Overrun Counts of Untuned Virtualization}
	\label{tab:latency_overrun_msw_new_virt}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Overruns} \\ \hline
		2.536                 & 8.940                 & 707.622               & 43                \\ \hline
	\end{tabular}
\end{table}

\subsection{Initial Comparison}
\noindent Comparing the latency values of the virtualization to those of bare metal in Figure~\ref{fig:gnuplot_max_latency_combined}, it is evident that there is a significant initial gap in latency. A maximum latency of 707.62~µs is not tolerable for a real-time virtualization and needs to be tuned.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/implementation/gnuplot_combined_max_latency.png}
	\caption[Initial Comparison of Latency Distribution]{Initial Comparison of Latency Distribution between Bare Metal and Virtualization}
	\label{fig:gnuplot_max_latency_combined}
\end{figure}

\clearpage

\section{Real-Time Performance Tunings}\label{sec:real-time_tunings}

In this section, the initial large gap in latency between the bare metal and the virtualized system is tackled. For this reason, an extensive tuning process is carried out. This involves configurations spanning the BIOS, kernel, host OS, QEMU/KVM virtualization layer, and the Salamander 4 guest OS itself. The individual configurations are discussed in detail, and the modifications are justified with a clear explanation. The goal is to bring the latency of the virtualization closer to that of the bare metal, thereby ensuring deterministic behavior under real-time constraints.

\bigskip \noindent It is important to mention that a real-time system implies determinism at every component of the system, ranging from hardware over the kernel to the software and the application on it. As already established earlier, the guest OS Salamander 4 runs Xenomai and is hence real-time capable. The host system also needs to be aware of the real-time determinism in order to achieve the highest possible reliability. The very first step of achieving the goal of reducing latency is to apply the PREEMPT-RT patch to the host system. This patch is a set of modifications to the Linux kernel with the goal of making it fully preemptible. This means almost all parts of the kernel can be preempted, and higher-priority tasks can interrupt lower-priority ones. This significantly reduces the time it takes for high-priority tasks to start executing after an event~\cite{RealtimeKernelPatchset}. The patch also includes support for high-resolution timers and makes the system more predictable~\cite{lutsykPipelinedMulticoreMachine2020}. Determinism and predictability are key for real-time applications where the timing of operations must be guaranteed with precise timing and scheduling~\cite{rostedtInternalsRTPatch2007}.

\bigskip \noindent Nevertheless, a real-time kernel alone does not make a system truly “real-time”~\cite{WhatRealtimeLinuxa}. Additional modifications are required to achieve this. The next subsections will deal with these real-time performance tunings. After each tuning, the \texttt{latency} test of Xenomai is executed to evaluate the improved latency. The most important metric of a latency test is, without a doubt, the worst latency value, because the system is only as reliable as its worst measurement. Since the period of the tests is set to 100 µs, there are approximately 10,000 samples per second. Therefore, 10 minutes or 600 seconds correspond to approximately 6,000,000 samples.

\clearpage

\subsection{BIOS Configurations}\label{subsec:bios_configurations}

BIOS stands for Basic Input/Output System. It abstracts the hardware and enables basic functions of a computer during the booting process, such as starting the operating system and loading other software. Since the BIOS is embedded very deep, its configuration can significantly influence the real-time performance of the system. Table~\ref{tab:bios_configuration} illustrates the specific BIOS settings that have been adjusted for the purpose of real-time performance. As an important note, this is not a definitive or complete list. Other devices may have additional settings that can be modified to achieve even better latency.

\begin{table}[H]
	\centering
	\caption{BIOS Configurations for Real-Time Performance}
	\label{tab:bios_configuration}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|l|l|}
			\hline
			\textbf{Option}               & \textbf{Status} \\
			\hline
			Hyper Threading               & Disabled        \\
			\hline
			Intel SpeedStep®              & Disabled        \\
			\hline
			Intel® Speed Shift Technology & Disabled        \\
			\hline
			C-States                      & Disabled        \\
			\hline
			VT-d                          & Enabled         \\
			\hline
		\end{tabular}}
\end{table}

\noindent In the following, these settings along with their impact on system latency are briefly described.

\begin{itemize}
	\item \textbf{Hyper Threading}: When hyper-threading is enabled in the BIOS settings, CPUs can work on two threads simultaneously instead of just one. This allows the parallelization of tasks and increases performance. However, in a real-time system like the guest Salamander 4, this can lead to increased latencies due to contention between threads. In order to ensure more deterministic behavior in the guest, it is disabled on the host.
	\item \textbf{Intel SpeedStep}: This dynamically adjusts the clock speed of the CPU based on workload. These dynamic adjustments of the speed can lead to unpredictable latencies in a real-time system. It is also disabled on the host to maintain a constant CPU speed.
	\item \textbf{Intel® Speed Shift Technology}: Similar to SpeedStep, Speed Shift allows the processor to directly control its frequency and voltage. This can lead to unpredictable latencies, too. Hence, it is also disabled on the host.
	\item \textbf{C States}: These are low-power idle states where the clock frequency and voltage of the CPU are reduced. Transitioning between C-states can cause variable latencies. To prevent this from happening, C-states are disabled on the host.
	\item \textbf{VT-d}: Direct access to physical devices from within virtual machines is possible when VT-d is enabled on the host. This can help reduce latencies associated with I/O operations in the virtual machine. It is therefore enabled on the host.
\end{itemize}

\noindent After applying the PREEMPT-RT patch and first real-time tunings in the BIOS settings, the \texttt{latency} test was run to see whether this had any impact on the determinism of the system. Figure~\ref{fig:gnuplot_max_latency_taskset} shows the latency values after tuning BIOS configurations.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/implementation/gnuplot_max_latency_taskset.png}
	\caption[Latency Distribution of Salamander 4 after BIOS Configurations]{Latency Distribution of Salamander 4 Virtualization after tuning BIOS Configurations}
	\label{fig:gnuplot_max_latency_taskset}
\end{figure}

\noindent The statistics obtained from this measurement are provided in Table~\ref{tab:latency_stats_virt_bios} and Table~\ref{tab:latency_overrun_msw_new_virt_bios}.

\begin{table}[H]
	\centering
	\caption{Latency Parameters after BIOS Configurations}
	\label{tab:latency_stats_virt_bios}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Param} & \textbf{Samples} & \textbf{Average (µs)} & \textbf{Std Dev (µs)} \\ \hline
		\textbf{min}   & 599              & 1.419                 & 0.507                 \\ \hline
		\textbf{avg}   & 5,999,885        & 3.398                 & 1.251                 \\ \hline
		\textbf{max}   & 599              & 74.015                & 26.590                \\ \hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption[Latency Statistics with Overrun Counts after BIOS Configurations]{Minimum, Average, and Maximum Latency with Overrun Counts after BIOS Configurations}
	\label{tab:latency_overrun_msw_new_virt_bios}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Overruns} \\ \hline
		0.969                 & 3.948                 & 457.545               & 22                \\ \hline
	\end{tabular}
\end{table}

\noindent The maximum latency value recorded was 457.545~µs. When compared to the initial worst latency of 707.62~µs, this is a step for the better, but certainly still not acceptable within this work's definition of hard real-time determinism. Besides, the number of overruns decreased from 43 to 22.

\subsection{Kernel Configurations}\label{subsec:kernel_configurations}
The kernel command-line parameters are shown in Code~\ref{script:kernel_cli} below. To access and modify them, the file is located in \texttt{/etc/default/grub}.

\vspace{2em}
\begin{lstlisting}[name={Kernel Configurations for Real-Time Performance},label={script:kernel_cli}]
		GRUB_CMDLINE_LINUX="isolcpus=4 rcu_nocbs=4 rcu_nocb_poll nohz_full=4 default_hugepagesz=1G hugepagesz=1G hugepages=8 intel_iommu=on rdt=l3cat nmi_watchdog=0 idle=poll clocksource=tsc tsc=reliable audit=0 skew_tick=1 intel_pstate=disable intel.max_cstate=0 intel_idle.max_cstate=0 processor.max_cstate=0 processor_idle.max_cstate=0 nosoftlockup no_timer_check nospectre_v2 spectre_v2_user=off kvm.kvmclock_periodic_sync=N kvm_intel.ple_gap=0 irqaffinity=0"
\end{lstlisting}

\noindent In the following, these settings along with their impact on system latency are briefly described.

\begin{itemize}
	\item \textbf{isolcpus=4}: Isolates CPU 4 from the general scheduler, meaning no process will be scheduled to run on this CPU unless it is explicitly assigned. CPU isolation is explained in detail in Subsubsection~\ref{subsubsec:cpu_isolation}.
	\item \textbf{rcu\_nocbs=4}: The Linux kernel uses a synchronization mechanism called RCU, or Read-Copy-Update. It lets writers update the data in a way that guarantees readers will always see the same version while enabling multiple readers to access shared data without locks. The RCU subsystem uses callback functions that need to be invoked once readers are done with the data they accessed. By default, these callbacks are handled by the CPUs that executed the read-side critical sections. This parameter offloads RCU callback handling from CPU 4 to other CPUs. CPU 4 remains dedicated to high-priority tasks, which helps in reducing latency.
	\item \textbf{rcu\_nocb\_poll}: This is used together with \texttt{rcu\_nocbs} and causes the system to actively poll for RCU callbacks to invoke, instead of waiting for the next RCU grace period. This reduces latency.
	\item \textbf{nohz\_full=4}: Makes CPU core 4 "tickless", meaning the kernel tries to avoid sending periodic scheduling-clock interrupts to the CPU when there are no runnable tasks. This lowers latency by reducing unnecessary wake-ups but may increase power consumption because the CPU is not able to enter a low-power state when idle. Additionally, timer interrupts cannot be fully eliminated because certain events, such as incoming interrupts or task activations, can still cause the kernel to send timer interrupts to the tickless CPU.
	\item \textbf{default\_hugepagesz=1G, hugepagesz=1G, hugepages=8}: Huge pages are large contiguous areas of memory that can be used by applications and the kernel, instead of the traditional 4KB small pages. The default huge page size is set to 1GB, and 8 huge pages of 1GB size are reserved at boot. This pre-allocation makes sure that these large memory regions are available to be used by the kernel or applications without having to dynamically allocate and potentially fail.
	\item \textbf{intel\_iommu=on}: Enables Intel's IOMMU (Input/Output Memory Management Unit), which connects a DMA (Direct Memory Access)-capable I/O bus to the main memory. It allows these devices to directly access and use memory, which is especially helpful for virtualization and device passthrough. 
	\item \textbf{rdt=l3cat}: Activates the L3 CAT (L3 Cache Allocation Technology) feature of Intel's RDT (Resource Director Technology). Unlike L1 and L2 caches, where each core has its fixed capacity, L3 cache is a shared pool among multiple cores. L3 CAT controls the amount of L3 cache that a process can use. By controlling cache allocation, it can prevent a single process from monopolizing the L3 cache, which is particularly beneficial in virtualized environments where multiple virtual machines share the same physical host.
	\item \textbf{nmi\_watchdog=0}: Disables the NMI (Non-Maskable Interrupt) watchdog, which is a debugging feature of the Linux kernel. It works by periodically generating non-maskable interrupts. If the system does not respond to these interrupts within a certain timeframe, the NMI watchdog concludes that the system has hung and generates a system dump for debugging. This constant monitoring consumes CPU cycles and can introduce undesirable latency in real-time systems.
	\item \textbf{idle=poll}: Changes the CPU's idle loop behavior to active polling. Instead of entering a low-power state when idle, the CPU continuously polls for new tasks. This can reduce task start latency in real-time systems, but it increases power consumption.
	\item \textbf{clocksource=tsc, tsc=reliable}: TSC (Time Stamp Counter) is a high-resolution timer provided by most x86 processors that counts the number of CPU cycles since it was last reset. Accurate timekeeping is crucial, particularly for real-time systems. These parameters set the clocksource to TSC and mark it as a reliable source of timekeeping, meaning it increments at a consistent rate and does not stop when the processor is idle.
	\item \textbf{audit=0}: Disables the Linux audit system. When it is enabled, it generates log entries for security-relevant events, which is a slow operation since they are written to disk. If there are a large number of such events, the audit system can consume significant CPU time and I/O bandwidth, which could lead to higher latency.
	\item \textbf{skew\_tick=1}: Enables a mode in the Linux kernel that reduces timer interrupt overhead. Normally, timer interrupts happen simultaneously on all CPUs, resulting in all CPUs exiting their low-power states at once. This can lead to increased contention for system resources. When enabled, the kernel offsets the timer interrupts on different CPUs, spreading them out over time.
	\item \textbf{intel\_pstate=disable}: Disables the Intel P-state driver, which is a part of the Linux kernel that handles power management for Intel CPUs. It controls the frequency of the CPU by scaling it up when demand is high and scaling it down to save power when demand is low. This dynamic frequency scaling is disabled because it leads to increased latencies for real-time systems.
	\item \textbf{intel.max\_cstate=0, intel\_idle.max\_cstate=0, processor.max\_cstate=0, processor\_idle.max\_cstate=0}: These parameters disable deeper C-states (CPU power saving states). Normally, when a CPU is idle, it can enter various C-states, with higher-numbered states representing deeper sleep states that save more power but take longer to wake up from. In real-time systems, these wake-up delays can be problematic. Disabling them keeps the CPUs ready to respond quickly to new tasks and helps in reducing latency.
	\item \textbf{nosoftlockup}: Disables the soft lockup detector in the Linux kernel. A soft lockup is when a CPU is busy executing kernel code for a long period of time without giving other tasks a chance to run. Especially threads with SCHED\_FIFO policy occupy the CPU for an extensive duration. This is detected and reported by the soft lockup detector, hence it is disabled to prevent these unnecessary warnings.
	\item \textbf{no\_timer\_check}: Disables the check for broken timer interrupt sources. Broken timer interrupt sources are problems with hardware or software that prevent timer interrupts from working as intended. Such a timer may not generate interrupts at the expected rate or at all. The kernel skips the checks for these broken timer interrupt sources, which can cause unnecessary overhead in a real-time system where every CPU cycle counts.
	\item \textbf{nospectre\_v2, spectre\_v2\_user=off}: These parameters disable mitigations for the Spectre v2 vulnerability. Spectre v2 is a hardware vulnerability that affects many modern microprocessors and can allow malicious programs to access sensitive data they are not supposed to. While this is necessary for security, it has an impact on performance and should be turned off in controlled environments where the risk of exploitation is low.
	\item \textbf{kvm.kvmclock\_periodic\_sync=N, kvm\_intel.ple\_gap=0}: These are KVM (Kernel-based Virtual Machine) related parameters. They disable the periodic synchronization of the kvmclock and set the gap between PLE (Pause Loop Exiting) events to 0. The kvmclock is a paravirtualized clock source provided by KVM to its guest OS and disabling this reduces latency introduced by clock synchronization. By setting the gap to 0, the virtual machine exits the pause loop immediately, which can reduce latency in spinlock-intensive workloads.
	\item \textbf{irqaffinity=0}: Sets the default IRQ affinity mask so that interrupts are handled by non-isolated CPUs. This means that no CPU core is preferred over another for handling IRQs. Instead, it lets the operating system decide how to distribute these IRQs across all the CPUs. Subsubsection~\ref{subsubsec:irq_handling} dives deeper into interrupt request affinity. 
\end{itemize}

\clearpage

\noindent After the kernel modifications on top of the BIOS settings from the last subsection, Figure~\ref{fig:gnuplot_max_latency_rt_kernelparam} demonstrates the latency values gathered in 10 minutes.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/implementation/gnuplot_max_latency_rt_kernelparam.png}
	\caption[Latency Distribution of Salamander 4 after Kernel Configurations]{Latency Distribution of Salamander 4 Virtualization after tuning Kernel Configurations}
	\label{fig:gnuplot_max_latency_rt_kernelparam}
\end{figure}

\noindent The statistics obtained from this measurement are provided in Table~\ref{tab:latency_stats_kernelparam} and Table~\ref{tab:latency_overrun_msw_new_virt_kernel}.

\begin{table}[H]
	\centering
	\caption{Latency Parameters after Kernel Configurations}
	\label{tab:latency_stats_kernelparam}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Param} & \textbf{Samples} & \textbf{Average (µs)} & \textbf{Std Dev (µs)} \\ \hline
		\textbf{min}   & 599              & 3.356                 & 0.551                 \\ \hline
		\textbf{avg}   & 5,999,972        & 4.036                 & 0.290                 \\ \hline
		\textbf{max}   & 599              & 13.484                & 1.454                 \\ \hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption[Latency Statistics with Overrun Counts after Kernel Configurations]{Minimum, Average, and Maximum Latency with Overrun Counts after Kernel Configurations}
	\label{tab:latency_overrun_msw_new_virt_kernel}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Overruns} \\ \hline
		2.545                 & 4.811                 & 21.694                & 0                 \\ \hline
	\end{tabular}
\end{table}

\noindent There is a significant improvement in the latency of the virtualized Salamander 4 OS. With a worst latency value of 21.694~µs, the real-time tunings appear to be working as desired. Additionally, no overruns occurred during the latency test. This means that the system consistently stayed within the required time constraints

\clearpage

\subsection{Host OS Configurations}\label{subsec:host_configurations}
The host OS needs to provide an environment where the guest OS can operate in real-time. This entails a number of host-side adjustments to reduce interruptions and latency. In the following, a detailed overview of these configurations and their impact on the real-time performance of the Salamander 4 guest OS is provided.

\subsubsection{CPU Affinity and Isolation}\label{subsubsec:cpu_isolation}
Isolating CPUs means removing all user-space threads and unbound kernel threads, since bound kernel threads are tied to specific CPUs and hence cannot be moved. For CPU isolation, the \texttt{isolcpus} function is used to isolate a CPU from the general scheduling algorithms of the operating system. This means that the isolated CPUs will not be used for regular task scheduling, allowing them to be dedicated for the real-time specific tasks. However, the \texttt{isolcpus} function only isolates at the user level and does not affect kernel tasks. Consequently, these kernel tasks and interrupts can still use the CPU~\cite{maPerformanceTuningKVMbased2013}, including systemd services. To prevent systemd services from running on an isolated CPU, the CPU affinity can be set with the line \texttt{CPUAffinity=0 1 2 3 5} in the \texttt{/etc/systemd/system.conf} file to indicate the CPUs that systemd services are allowed to run on. Every CPU other than the isolated CPU 4 is allowed. Code~\ref{output:pse} shows the user and kernel tasks that run on CPU 4. An isolated CPU can be bound to a task by using \texttt{taskset -c <CPU>}, as shown later in Subsection~\ref{subsec:qemu_configurations}. After the isolation, user tasks other than the QEMU process have been removed from running on this CPU. Only a few per-CPU kernel threads that are tied to this CPU still take CPU time.

\vspace{2em}
\begin{lstlisting}[name={User and Kernel Tasks on the isolated CPU},label={output:pse},escapeinside={(*@}{@*)}]
		sigma_ibo@sigma-ibo:~$ cat /sys/devices/system/cpu/isolated
		4
		sigma_ibo@sigma-ibo:~$ ps axHo psr,pid,lwp,args,policy,nice,rtprio | awk '$1 == 4'
		4      38      38 [cpuhp/4]                   TS    0      -
		4      39      39 [idle_inject/4]             FF    -     50
		4      40      40 [migration/4]               FF    -     99
		4      41      41 [ksoftirqd/4]               TS    0      -
		4      42      42 [kworker/4:0-events]        TS    0      -
		4      43      43 [kworker/4:0H-kblockd]      TS  -20      -
		4     153     153 [kworker/4:1-events]        TS    0      -
		4   81649   81649 qemu-system-x86_64 -M pc,ac TS    0      -
		4   81649   81654 qemu-system-x86_64 -M pc,ac TS    0      -
		4   81649   81676 qemu-system-x86_64 -M pc,ac TS    0      -
		4   81649   81702 qemu-system-x86_64 -M pc,ac TS    0      -
		4   81649   82185 qemu-system-x86_64 -M pc,ac TS    0      -
		4   81649   82187 qemu-system-x86_64 -M pc,ac TS    0      -
		4   82134   82134 [kworker/4:1H-kblockd]      TS  -20      -
\end{lstlisting}

\subsubsection{Interrupt Affinity}\label{subsubsec:irq_handling}
Once the CPUs are isolated, interrupt request handling is the next step. The purpose of interrupt requests is to inform the CPU to stop working on a certain job and start working on another. This allows hardware devices to communicate with the CPU through frequent context switches, which can introduce latency in real-time systems. All the existing interrupts can be monitored using the command \texttt{watch -d -n 1 cat /proc/interrupts} to observe changes in the interrupt requests handled by each CPU in real-time. The IRQs need to be removed from the isolated CPU by manipulating the \texttt{/proc/irq/<IRQ>/smp\_affinity} files. The value in the \texttt{smp\_affinity} file is a bitmask in hexadecimal format where each bit corresponds to a CPU. The Least Significant Bit (LSB) on the right corresponds to the first CPU (CPU0), and the Most Significant Bit (MSB) on the left corresponds to the last CPU (CPU5). When there are 6 CPUs available, the default value for \texttt{smp\_affinity} would be 3F (11 1111). Removing CPU 4 out of this bitmask would be setting bit five to zero, resulting in 2F (10 1111). Looking individually in the \texttt{/proc/irq/<IRQ>/smp\_affinity} files for every IRQ would be time-consuming. The Python script in Code~\ref{script:smp_affinity} was written to show a table of the distribution of interrupt requests across each CPU, since the \texttt{/proc/interrupts} file only shows if an interrupt has occurred and does not indicate which CPUs are capable of serving which IRQs. The program first reads the \texttt{smp\_affinity} files for each IRQ from the \texttt{/proc/irq} directory and stores the CPUs assigned to handle each IRQ in a dictionary. It then creates a table showing the distribution of interrupt requests across all CPUs.

\bigskip \noindent As the \texttt{proc} filesystem resets to its default state after each reboot, manually changing numerous IRQ files through the said bitmasks is tedious and time-consuming. This process can be automated using the shell script in Code~\ref{script:change_smp_affinity}, which can automatically be executed after every reboot.

\clearpage

\vspace*{-5em}
\begin{lstlisting}[name={Code to show IRQ distribution across each CPU},label={script:smp_affinity}]
		import os
		import pandas as pd
		from tabulate import tabulate
		
		# Get the number of CPUs
		num_cpus = os.cpu_count()
		
		# Initialize a dictionary to store the CPUs for each IRQ
		irqs = {}
		
		# Iterate over each IRQ
		for irq in os.listdir('/proc/irq'):
			# Check if the smp_affinity file exists for this IRQ
			if os.path.isfile(f'/proc/irq/{irq}/smp_affinity'):
				# Read the current smp_affinity
				with open(f'/proc/irq/{irq}/smp_affinity', 'r') as f:
					affinity = int(f.read().strip(), 16)
				# Initialize an empty list to store the CPUs for this IRQ
				cpus = []
				# Iterate over each CPU
				for cpu in range(num_cpus):
					# Check if the bit for the current CPU is set
					if ((affinity & (1 << cpu)) != 0):
						# Add the CPU to the list for this IRQ
						cpus.append(cpu)
				# Sort the list of CPUs
				cpus.sort()
				# Add the list of CPUs to the dictionary for this IRQ
				irqs[irq] = cpus
		
		# Create a DataFrame to store the table
		df = pd.DataFrame(index=sorted(irqs.keys(), key=int), columns=range(num_cpus))
		
		# Fill the DataFrame with 'x' where a CPU is assigned to an IRQ
		for irq, cpus in irqs.items():
			for cpu in cpus:
				df.loc[irq, cpu] = 'x'
		
		# Replace NaN values with empty strings
		df.fillna('', inplace=True)
		
		# Print the table in pipe format
		print(tabulate(df, headers='keys', tablefmt='pipe', showindex=True))
		
		# Convert the DataFrame to a markdown table
		markdown_table = df.to_markdown()
		
		# Write the markdown table to a file
		with open('table_CPU_IRQ.md', 'w') as f:
			f.write(markdown_table)
\end{lstlisting}

\clearpage

\vspace*{-5em}
\begin{lstlisting}[name={Script to change IRQ Assignment of a CPU},label={script:change_smp_affinity}]
		#!/bin/bash

		# Check if a command-line argument is provided
		if [ -z "$1" ]; then
			echo "Please provide a CPU number as a command-line argument."
			exit 1
		fi
		
		# Get the CPU number from the command-line argument
		CPU=$1
		
		# Define the mask values
		declare -A mask_values
		mask_values=( [0]="3ffe" [1]="3ffd" [2]="3ffb" [3]="3ff7" [4]="3fef" [5]="3fdf" [6]="3fbf" [7]="3f7f" [8]="3eff" [9]="3dff" [10]="3bff" [11]="37ff" [12]="2fff" [13]="17ff")
		
		# Run the check_smp_affinity.sh script and get the IRQs
		IRQs=$(./check_smp_affinity.sh $CPU | grep -o '[0-9]\+')
		
		# Initialize an empty array to store the IRQs that could not be removed
		failed_IRQs=()
		# Initialize an empty array to store the IRQs that were successfully removed
		succeeded_IRQs=()
		
		# Loop over the IRQs
		for IRQ in $IRQs; do
			# Try to change the smp_affinity
			echo ${mask_values[$CPU]} | sudo tee /proc/irq/$IRQ/smp_affinity > /dev/null 2>&1
			# If the command failed, add the IRQ to the failed_IRQs array
			if [ $? -ne 0 ]; then
				failed_IRQs+=($IRQ)
			else
				succeeded_IRQs+=($IRQ)
			fi
		done
		
		# Check if there were any failed IRQs
		if [ ${#failed_IRQs[@]} -ne 0 ]; then
			echo "IRQs ${failed_IRQs[@]} could not be removed from CPU $CPU."
		fi
		
		# Check if there were any successful IRQs
		if [ ${#succeeded_IRQs[@]} -ne 0 ]; then
			# Remove the first entry from the succeeded_IRQs array
			succeeded_IRQs=("${succeeded_IRQs[@]:1}")
			echo "IRQs ${succeeded_IRQs[@]} were removed from CPU $CPU."
		fi		
\end{lstlisting}

\subsubsection{RT-Priority}
Having a real-time kernel itself is an indispensable step of achieving deterministic behavior, but not enough by itself to take full advantage of the real-time capabilities. One key aspect of this is real-time priorities, thoroughly explained by Richard Weinberger~\cite{LinuxProcessPriorities}. In essence, Table~\ref{tab:scheduling_priorities} lists minimum and maximum priorities for different scheduling policies in Linux.

\begin{table}[H]
	\centering
	\caption{Minimum and Maximum Priorities for Different Scheduling Policies}
	\label{tab:scheduling_priorities}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Scheduling Policy} & \textbf{Min Priority} & \textbf{Max Priority} \\ \hline
			SCHED\_OTHER               & 0                     & 0                     \\ \hline
			SCHED\_FIFO                & 1                     & 99                    \\ \hline
			SCHED\_RR                  & 1                     & 99                    \\ \hline
			SCHED\_BATCH               & 0                     & 0                     \\ \hline
			SCHED\_IDLE                & 0                     & 0                     \\ \hline
			SCHED\_DEADLINE            & 0                     & 0                     \\ \hline
		\end{tabular}}
\end{table}

\noindent \texttt{SCHED\_FIFO} allows deterministic, high-priority execution of critical tasks without being preempted by lower-priority processes. The virtual machine can either be started with \texttt{chrt -f <PRIO>} or adjusted at a later point with \texttt{chrt -f <PRIO> <PID>}. It is also important that there are no other unexpected real-time processes running on the system concurrently.

\subsubsection{Disable RT Throttling}
If a real-time task consumes 100\% of the CPU time, the system may become unresponsive as a whole. This happens because an RT process is constantly using the CPU and the Linux scheduler will not schedule other non-RT processes in the meantime. To prevent complete system lockups, the kernel has a function to throttle RT processes if they consume 0.95 seconds out of every 1 second of CPU time. It does this by pausing the process for the remaining 0.05 seconds, which is not desired because this could result in missed deadlines. RT throttling can be disabled by writing the value "-1" to the \texttt{/proc/sys/kernel/sched\_rt\_runtime\_us} file. This change also needs to be made permanent because the \texttt{proc} filesystem resets to its default state after each reboot. For this purpose, the line \texttt{kernel.sched\_rt\_runtime\_us = -1} can be appended to the end of the \texttt{/etc/sysctl.conf} file, which is read at boot time and used to configure kernel parameters. This reduces the potential for missed deadlines.

\clearpage

\subsubsection{Disable Timer Migration}
Timer migration allows timers to be moved from one CPU to another, which means the kernel can balance load across multiple CPUs. In a real-time system, this can introduce latency and jitter. To disable timer migration, the value ``0'' needs to be written to the \texttt{/proc/sys/kernel/timer\_migration} file. This change also needs to be made permanent by writing the line \texttt{kernel.timer\_migration = 0} to the \texttt{/etc/sysctl.conf} file. This reduces the amount of context switches and interrupts.

\subsubsection{Set Device Driver Work Queue}
The device driver work queue allows time-consuming tasks to be offloaded to be processed later in a separate kernel thread. By setting the work queue away from CPU 4 to another CPU, it is free to handle real-time tasks without being interrupted by these work queue tasks. This is done by specifying a bitmask to exclude CPU 4 in the files \texttt{/sys/devices/virtual/workqueue/cpumask} and \texttt{/sys/bus/workqueue/devices/writeback/cpumask}. In this case, that is 2F.

\subsubsection{Disable RCU CPU Stall Warnings}
As already mentioned in Section~\ref{subsec:kernel_configurations}, the Linux kernel uses RCU as a synchronization mechanism for reading from and writing to shared data. An RCU CPU stall in the Linux kernel can occur due to several reasons. These include a CPU looping in an RCU read-side critical section, a CPU looping with interrupts disabled, a CPU looping with preemption disabled, or a CPU not getting around to less urgent tasks, known as “bottom halves”. The number of seconds the kernel should wait before checking for stalled CPUs and reporting a stall warning can be set via the \texttt{/sys/module/rcupdate/parameters/rcu\_cpu\_stall\_timeout} file. These warnings can be suppressed altogether to reduce the potential for increased latency by writing ``1'' to the \texttt{/sys/module/rcupdate/parameters/rcu\_cpu\_stall\_suppress} file. This setting is also not persistent across reboots, so the command needs to be added to a startup script.

\clearpage

\subsubsection{Stop Services}
Table~\ref{tab:stop_servies} presents services that can be further sources for random latency and unnecessary overhead. Stopping these services through \texttt{sudo systemctl stop <SERVICE>} prevents them from interrupting the CPU with their tasks.

\begin{table}[H]
	\centering
	\caption{Services that are stopped to reduce random Latency and Overhead}
	\label{tab:stop_servies}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|l|l|}
			\hline
			\textbf{Service}        & \textbf{Description}                        \\
			\hline
			irqbalance.service      & Distributes hardware interrupts across CPUs \\\hline
			thermald.service        & A daemon that prevents overheating          \\\hline
			wpa\_supplicant.service & A service for wireless network devices      \\
			\hline
		\end{tabular}}
\end{table}

\subsubsection{Disable Machine Check}
Machine checks report hardware errors and these checks can cause interruptions and increase latency. Hence, it is best to disable them in real-time scenarios by writing a ``0'' to the \texttt{/sys/devices/system/machinecheck/machinecheck0/check\_interval} file.

\subsubsection{Boot into Text-Based Environment}
The Graphical User Interface (GUI) consumes great amounts of system resources. It often runs different background processes that are not essential for real-time systems and hence increase latency. These resources can be freed up by switching to a text-based environment. This way, processing power and memory can be allocated to critical real-time tasks which in turn can lead to lower latency and deterministic behavior. The command \texttt{systemctl set-default multi-user.target} and a following reboot allow for booting into a text-based environment. For the sake of completeness, the graphical interface can be brought back through the command \texttt{systemctl set-default graphical.target} and a following reboot.

\clearpage

\noindent After these comprehensive host configurations, the result of the \texttt{latency} test is depicted in Figure~\ref{fig:max_latency_rt_kernelparam_host}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/implementation/gnuplot_max_latency_rt_kernelparam_host.png}
	\caption[Latency Distribution of Salamander 4 after Host Configurations]{Latency Distribution of Salamander 4 Virtualization after tuning Host Configurations}
	\label{fig:max_latency_rt_kernelparam_host}
\end{figure}

\noindent The statistics obtained from this measurement are provided in Table~\ref{tab:latency_stats_virt_host} and Table~\ref{tab:latency_overrun_msw_new_virt_host}.

\begin{table}[H]
	\centering
	\caption{Latency Parameters for Host Configurations}
	\label{tab:latency_stats_virt_host}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Param} & \textbf{Samples} & \textbf{Average (µs)} & \textbf{Std Dev (µs)} \\ \hline
		\textbf{min}   & 599              & 2.998                 & 0.242                 \\ \hline
		\textbf{avg}   & 5,999,972        & 4.043                 & 0.255                 \\ \hline
		\textbf{max}   & 599              & 12.124                & 1.828                 \\ \hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption[Latency Statistics with Overrun Counts after Host Configurations]{Minimum, Average, and Maximum Latency with Overrun Counts after Host Configurations}
	\label{tab:latency_overrun_msw_new_virt_host}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Overruns} \\ \hline
		2.591                 & 4.834                 & 18.441                & 0                 \\ \hline
	\end{tabular}
\end{table}

\noindent A worst latency value of 18.441~µs shows that the virtualized Salamander 4 OS got even more reliable with the host configurations.

\clearpage

\subsection{QEMU/KVM Configurations}\label{subsec:qemu_configurations}
KVM allows the guest OS to run directly on the hardware, bypassing the need for traditional emulation which can introduce delays. QEMU, when used with KVM, provides hardware-assisted virtualization, which also lowers latency in the guest OS.

\subsubsection{Tune LAPIC Timer Advance}
The Local Advanced Programmable Interrupt Controller (LAPIC) is a built-in timer that handles the delivery of interrupts to the CPU. It generates interrupts at a rate. This rate can be tuned in the \texttt{/sys/module/kvm/parameters/lapic\_timer\_advance\_ns} file to reduce the frequency of interrupts and therefore decrease the latency of the guest VM. The default value is ``-1'', which means that the kernel will automatically calculate an appropriate advance for the timer. Here, it is set to the value ``7500''. Hence, the timer interrupt will be delivered 7500 nanoseconds earlier than it is actually due. This gives the VM more time to handle it.

\subsubsection{Set QEMU Options for Real-Time VM}
QEMU provides options that can be used to improve the real-time performance of the guest VM. Table~\ref{tab:qemu_options} briefly explains these options.

\begin{table}[H]
	\centering
	\caption{QEMU Options for Real-Time Performance}
	\label{tab:qemu_options}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|p{6.5cm}|p{8cm}|}
			\hline
			\textbf{QEMU Option}                                                             & \textbf{Description}                                                             \\\hline
			\texttt{-object memory-backend-ram,}\newline\texttt{id=ram0,size=4G,prealloc=on} & Locks the memory of the VM to 4GB and prevents it from being swapped out to disk \\\hline
			\texttt{-mem-prealloc} \newline \texttt{-mem-path /dev/hugepages/}               & Enables the use of hugepages and improves \newline memory access                 \\\hline
		\end{tabular}}
\end{table}

\noindent Code~\ref{script:qemu_def_tuned} shows the final QEMU script used to start the Salamander 4 virtualization, including these options.

\clearpage

\begin{lstlisting}[name={Tuned QEMU Script for starting Salamander 4 Virtualization},label={script:qemu_def_tuned}]
		#!/bin/sh

		if  [ ! -d drive-c/ ]; then
				echo "Filling drive-c/"
				mkdir drive-c/
				tar -C drive-c/ -xf stek-drive-c-image-sigmatek-core2.tar.gz
		fi
			
		exec taskset -c 4 chrt -f 99 qemu-system-x86_64 -M pc,accel=kvm -kernel ./bzImage \
		-m 2048 -drive file=salamander-image-sigmatek-core2.ext4,format=raw,media=disk \
		-append "console=ttyS0 console=tty1 root=/dev/sda rw panic=1 sigmatek_lrt.QEMU=1 ip=dhcp rootfstype=ext4 schedstats=enable nohlt idle=poll quiet xeno_hal.smi=1 xenomai.smi=1 threadirqs" \
		-net nic,model=e1000,netdev=e1000 -netdev bridge,id=e1000,br=nm-bridge \
		-fsdev local,security_model=none,id=fsdev0,path=drive-c -device virtio-9p-pci,id=fs0,fsdev=fsdev0,mount_tag=/mnt/drive-C \
		-device vhost-vsock-pci,guest-cid=3,id=vsock0 \
		-drive if=pflash,format=qcow2,file=ovmf.code.qcow2 \
		-object memory-backend-ram,id=ram0,size=4G,prealloc=on \
		-mem-prealloc -mem-path /dev/hugepages \
		-no-reboot -nographic
\end{lstlisting}

\vspace{2em}
\noindent After QEMU configurations, the result of the \texttt{latency} test is depicted in Figure~\ref{fig:max_latency_rt_kernelparam_host_qemu}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/implementation/gnuplot_max_latency_rt_kernelparam_host_qemu.png}
	\caption[Latency Distribution of Salamander 4 after QEMU Configurations]{Latency Distribution of Salamander 4 Virtualization after tuning QEMU Configurations}
	\label{fig:max_latency_rt_kernelparam_host_qemu}
\end{figure}

\clearpage

\noindent The statistics obtained from this measurement are provided in Table~\ref{tab:latency_stats_virt_qemu} and Table~\ref{tab:latency_overrun_msw_new_virt_qemu}.

\begin{table}[H]
	\centering
	\caption{Latency Parameters after QEMU Configurations}
	\label{tab:latency_stats_virt_qemu}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Param} & \textbf{Samples} & \textbf{Average (µs)} & \textbf{Std Dev (µs)} \\ \hline
		\textbf{min}   & 599              & 3.125                 & 0.432                 \\ \hline
		\textbf{avg}   & 5,999,973        & 4.018                 & 0.291                 \\ \hline
		\textbf{max}   & 599              & 15.883                & 0.351                 \\ \hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption[Latency Statistics with Overrun Counts after QEMU Configurations]{Minimum, Average, and Maximum Latency with Overrun Counts after QEMU Configurations}
	\label{tab:latency_overrun_msw_new_virt_qemu}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Overruns} \\ \hline
		2.614                 & 4.779                 & 17.134                & 0                 \\ \hline
	\end{tabular}
\end{table}

\noindent The maximum latency value decreased slightly. More notably, the standard deviation of the maximum latency samples, with the test lasting 600 seconds and thus having 600 worst latencies for each second, also decreased from 1.828 µs to 0.351 µs. Although the difference is not much, when the latency values are this small, it becomes noticeable. This means the system is more stable and predictable, thanks to the real-time tunings performed up to this point. 

\subsection{Guest OS Configurations}\label{subsec:guest_configurations}
The guest OS, Salamander 4, is already based on Xenomai and uses the Cobalt real-time core with the Dovetail extension. Therefore, no additional modifications were necessary for the guest OS itself. The focus was primarily on optimizing the host and virtualization layer to achieve the desired real-time performance.

\subsection{Other Configurations}
There are other configurations that may be relevant depending on the case. Linux Kernel Developer Steven Rostedt explains all relevant aspects of a real-time system that must be considered in~\cite{kernelrecipesKernelRecipes20162016} and gives insights for finding sources of latency on the Linux system in~\cite{thelinuxfoundationFindingSourcesLatency2020}. A checklist for writing Linux real-time applications is provided by John Ogness in~\cite{thelinuxfoundationChecklistWritingLinux2020}. Every layer of the system stack must be deterministic to ensure predictable and reliable latency, including hardware, operating system, middleware and drivers, and the application software. ~\cite{HOWTOBuildRTapplication} and~\cite{RealtimeProgrammingLinux} describe the process of writing hard real-time Linux programs using the real-time preemption patch in great detail. Various hardware and software tunings are listed in~\cite{RealTimePerformanceTuning2022}~and~\cite{KVMQemuVirtualization}.

\clearpage

\section{Real-Time Robotic Application}\label{sec:robotic_application}
This chapter compares the latency of the Salamander 4 operating system before and after the real-time performance tunings with the latency of Salamander 4 running on bare metal. This comparison is done with the aid of a program that will be explained shortly. The goal is to understand how closely the performance of the virtualization can match that of the bare metal. The experimental setup includes a six-axis mini-robot, illustrated in Figure~\ref{fig:mini_robot}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\columnwidth]{img/experiment/mini_robot.jpg}
	\caption[Mini-Robot of the Experiment]{Mini-Robot of the Experiment~\cite{6DFRoboticArm}}
	\label{fig:mini_robot}
\end{figure}

\noindent The robot arm consists of six MG996R digital servo motors~\cite{MG996RServoMotor}, equipped with a metal gearbox. The motor is able to rotate in a range of approximately 180 degrees and its position can be controlled with a high degree of accuracy. Each servo motor has three wires that need to be connected as shown in Figure~\ref{fig:servo}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/experiment/servo.jpg}
	\caption[MG996R Servo Motor]{MG996R Servo Motor~\cite{PackMg996rMetal}}
	\label{fig:servo}
\end{figure}

\noindent To drive the motor, it has to be powered using the red and brown wires and can be controlled by sending PWM signals to the orange wire. A 1,500 µs pulse every 20 milliseconds centers the servo. A 1,000 µs pulse turns it 90 degrees left, and a 2,000 µs pulse turns it 90 degrees right. This is depicted in Figure~\ref{fig:servo_angle}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/experiment/servo_angle.png}
	\caption[Controlling the MG996R Servo Motor]{Controlling the MG996R Servo Motor~\cite{twierengEmilyQuadrupedDog}}
	\label{fig:servo_angle}
\end{figure}

\noindent In this experiment, this PWM signal is generated by the proprietary PW 022 pulse width module of Sigmatek~\cite{DigitalOutputSIGMATEK}. Its connector layout is illustrated in Figure~\ref{fig:pw022_connectors}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\columnwidth]{img/experiment/pw022_connectors.png}
	\caption[PW 022 Pulse Width Module Connector Layout]{PW 022 Pulse Width Module Connector Layout~\cite{DigitalOutputSIGMATEK}}
	\label{fig:pw022_connectors}
\end{figure}

\noindent The module has two +24 V switching PWM outputs with an adjustable frequency for controlling inductive loads. Since the mentioned servo motors operate between 4.8 volts and 7.2 volts~\cite{MG996RServoMotor}, this voltage needs to be reduced through resistors before supplying it to a servo motor. For this purpose, two resistors with resistances of 2500 ohms and 1000 ohms are connected in series. The voltage across each resistor is calculated using Ohm's law, which is given by equation~\ref{eq:ohms_law} below.
\begin{equation}
	\label{eq:ohms_law}
	V = I \cdot R
\end{equation}

\noindent In this equation, \(V\) represents the voltage across the resistor, \(I\) is the current flowing through the resistor, and \(R\) is the resistance of the resistor. The resulting voltages across the resistors are as follows:

\begin{itemize}
	\item The voltage across the 2.5 kiloohm resistor is approximately 17.14 volts.
	\item The voltage across the 1 kiloohm resistor is approximately 6.86 volts. This voltage is then supplied to the control wire of servo motor 1 of the mini-robot.
\end{itemize}

\noindent Connecting a second servo motor of the mini-robot means repeating the process of reducing the voltage through resistors for the second PWM output of the PW 022 module. The connection between the PW 022 module and the motor of the mini-robot is demonstrated in Figure~\ref{fig:pw022_minirobot_connection}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\columnwidth]{img/experiment/pw022_minirobot.png}
	\caption[Connection between PW 022 and Mini-Robot]{Connection between PW 022 and Mini-Robot~\cite{MG996RDigitalServo}}
	\label{fig:pw022_minirobot_connection}
\end{figure}

\noindent The program is written in Lasal Class 2 and is applied to all three mentioned versions of Salamander 4 to measure the reaction time of the robot to the commands. Prior to examining the software program, the next two subsections briefly explain the setup of each version of the experiment.

\subsection{Setup of Salamander 4 Bare Metal}
In this version, Salamander 4 runs on the CP 841 CPU unit, specifically designed for the Salamander 4 operating system. The PW 022 modules are directly mounted on the CPU via the S-DIAS bus~\cite{SDIASSIGMATEK}. There are three PW 022 modules because each can control two motors simultaneously, and the mini-robot has six motors. The setup is visible on Figure~\ref{fig:hardware_tree}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\columnwidth]{img/experiment/hardware_tree.png}
	\caption[Setup of Salamander 4 Bare Metal]{Setup of Salamander 4 Bare Metal}
	\label{fig:hardware_tree}
\end{figure}

\subsection{Setup of Salamander 4 Virtualization}\label{subsec:setup_experiment_virtualized}
In the virtualized setup of the experiment, the vCPU functionality of QEMU is used to connect QEMU with the PWM modules. In order to achieve that, the PCV 522 VARAN Manager PCI Insert Card~\cite{ControlsHMIsSIGMATEK}, which serves as a bridge between the PC and the rest of the setup, needs to be plugged into the PC. The command \texttt{lspci -nn} lists all PCI devices along with their vendor and device IDs. In this case, the command for binding the PCV 522 module to the VFIO-PCI driver is \texttt{sudo sh -c 'echo "5112 2200" > /sys/bus/pci/drivers/vfio-pci/new\_id'} with vendor ID \texttt{5112} and device ID \texttt{2200}.  To verify that the device has been successfully bound to the VFIO-PCI driver, \texttt{lspci -v} can be used. On top of the binding process, the QEMU script also needs to be modified to include the PCV 522 VARAN Manager PCI Insert Card in the virtualization. This is done by adding \texttt{-device\ vfio-pci,host=03:00.0 } to the QEMU script, where \texttt{03:00.0} refers to the PCI address of the device, with \texttt{03} being the bus number, \texttt{00} the device number, and \texttt{0} the function number. The final script can be seen in Code~\ref{script:qemu_def_pci}.

\clearpage

\vspace*{-5em}
\begin{lstlisting}[name={Final QEMU Script for Salamander 4 Virtualization with PCI Configuration},label={script:qemu_def_pci}]
		#!/bin/sh

		if  [ ! -d drive-c/ ]; then
				echo "Filling drive-c/"
				mkdir drive-c/
				tar -C drive-c/ -xf stek-drive-c-image-sigmatek-core2.tar.gz
		fi
			
		exec taskset -c 4 chrt -f 99 qemu-system-x86_64 -M pc,accel=kvm -kernel ./bzImage \
		-m 2048 -drive file=salamander-image-sigmatek-core2.ext4,format=raw,media=disk \
		-append "console=ttyS0 console=tty1 root=/dev/sda rw panic=1 sigmatek_lrt.QEMU=1 ip=dhcp rootfstype=ext4 schedstats=enable nohlt idle=poll quiet xeno_hal.smi=1 xenomai.smi=1 threadirqs" \
		-net nic,model=e1000,netdev=e1000 -netdev bridge,id=e1000,br=nm-bridge \
		-fsdev local,security_model=none,id=fsdev0,path=drive-c -device virtio-9p-pci,id=fs0,fsdev=fsdev0,mount_tag=/mnt/drive-C \
		-device vhost-vsock-pci,guest-cid=3,id=vsock0 \
		-drive if=pflash,format=qcow2,file=ovmf.code.qcow2 \
		-object memory-backend-ram,id=ram0,size=4G,prealloc=on \
		-mem-prealloc -mem-path /dev/hugepages \
		-device vfio-pci,host=03:00.0 \
		-no-reboot -nographic
\end{lstlisting}

\vspace{2em}

\noindent An additional Varan Connection module VI 021~\cite{InterfacesSplittersSIGMATEK} was required to enable the connection between the PCV 522 module and the PW 022 module that generates the signal to move the mini-robot. This setup is depicted in Figure~\ref{fig:virt_tree}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\columnwidth]{img/experiment/virt_tree.png}
	\caption[Setup of Salamander 4 Virtualization]{Setup of Salamander 4 Virtualization}
	\label{fig:virt_tree}
\end{figure}

\noindent Essentially, the CPU is being emulated by the vCPU functionality of QEMU, allowing the PW 022 modules to interact with the vCPU through the PCV 522 Insert Card and the VI 021 module as if they were communicating directly with a physical CPU unit.

\subsection{Robotic Application}

Robots operate in real-time, meaning commands must be executed within a set time frame. As explained earlier, there are hard real-time systems, which must always meet their deadlines, and soft real-time systems, which can occasionally miss them. If a robot misses these deadlines, it can cause unwanted movements and jumpy behavior. The most crucial part of latency distribution for determinism are the outliers. Even if a system's latency is as expected 99\% of the time, the remaining 1\% can be worse than all the other measurements combined. Looking only at the mean and standard deviation misses these systemic issues.

\bigskip \noindent The impacts of real-time performance tunings on the determinism and latency of Salamander 4 virtualization were already shown in the previous subsection. Here, these improvements are demonstrated using a robotic application. This application is written in Lasal Class 2, as mentioned in Section~\ref{sec:application_context}. The goal is to determine the time from command issuance to the signal reaching the PWM, excluding the 20ms signal period required by the servomotor to process the signal and initiate movement. The program sequence is shown in Figure~\ref{fig:robotic_application_flow}.

\vspace{2em}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[node distance = 0.35cm, auto, % Adjusted node distance
			block/.style={
					rectangle,
					draw,
					fill=orange!30,
					text width=35em,
					rounded corners,
					align=center,
					inner sep=0.5em % Adjusted inner separation
				},
			line/.style={draw, -latex'}
		]
		% Place nodes
		\node [block] (issue) {\scriptsize \textbf{Command Issuance}\\A command is sent to the system to execute a movement.};
		\node [block, below=of issue] (record1) {\scriptsize \textbf{Recording Command Time}\\The timestamp of the moment the command is issued is recorded.};
		\node [block, below=of record1] (detect) {\scriptsize \textbf{Detecting Command Arrival}\\The moment when the signal reaches the PWM module is detected.};
		\node [block, below=of detect] (record2) {\scriptsize \textbf{Recording Command Arrival Time}\\The timestamp of the moment the signal reaches the PWM module is recorded.};
		\node [block, below=of record2] (calculate) {\scriptsize \textbf{Calculating Command-to-PWM Latency}\\The latency is determined by the difference between the command issuance time and \\\vspace{-0.5em} the command arrival time at the PWM module.};		
		% Draw edges
		\path [line] (issue) -- (record1);
		\path [line] (record1) -- (detect);
		\path [line] (detect) -- (record2);
		\path [line] (record2) -- (calculate);
	\end{tikzpicture}
	\caption[Flowchart of Robotic Application]{Flowchart of Robotic Application}
	\label{fig:robotic_application_flow}
\end{figure}

\vspace{1em}

\noindent This latency measurement above is performed 1,000 times to get reliable results. These results are presented in the next subsubsections.

\clearpage

\subsubsection{Latency in Salamander 4 Bare Metal}\label{subsubsec:latency_hardware}
\noindent The latency values for bare metal Salamander 4 OS acquired from the program above are demonstrated in Figure~\ref{fig:rtos_latencies} and detailed in Table~\ref{tab:robotic_application_latency_values_hardware}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/implementation/rtos_latencies.png}
	\caption{Robotic Application Latency Values of Salamander 4 Bare Metal}
	\label{fig:rtos_latencies}
\end{figure}

\begin{table}[H]
	\centering
	\caption{Robotic Application Latency Statistics Bare Metal}
	\label{tab:robotic_application_latency_values_hardware}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{Samples} & \textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Std Dev (µs)} \\ \hline

			1000             & 1.211                 & 1.347                 & 1.49                  & 0.082                 \\ \hline
		\end{tabular}}
\end{table}

\clearpage

\subsubsection{Latency in Untuned Salamander 4 OS Virtualization}\label{subsubsec:latency_virtualization}
\noindent The latency values acquired for the untuned virtualization of Salamander 4 OS are demonstrated in Figure~\ref{fig:untuned_virt_latencies} and detailed in Table~\ref{tab:robotic_application_latency_values_virt_def}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/implementation/untuned_virt_latencies.png}
	\caption[Robotic Application Latency Values of Salamander 4 Untuned Virtualization]{Robotic Application Latency Values of Salamander 4 Untuned Virtualization}
	\label{fig:untuned_virt_latencies}
\end{figure}

\begin{table}[H]
	\centering
	\caption{Robotic Application Latency Statistics Untuned Virtualization}
	\label{tab:robotic_application_latency_values_virt_def}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{Samples} & \textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Std Dev (µs)} \\ \hline
			1000             & 3.1                   & 24.603                & 129.46                & 13.876                \\ \hline
		\end{tabular}}
\end{table}

\clearpage

\subsubsection{Latency in Tuned Salamander 4 OS Virtualization}\label{subsubsec:latency_virtualization}
\noindent The latency values acquired for the tuned virtualization of Salamander 4 OS are demonstrated in Figure~\ref{fig:tuned_virt_latencies} and detailed in Table~\ref{tab:robotic_application_latency_values_virt_tun}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/implementation/tuned_virt_latencies.png}
	\caption[Robotic Application Latency Values of Salamander 4 Tuned Virtualization]{Robotic Application Latency Values of Salamander 4 Tuned Virtualization}
	\label{fig:tuned_virt_latencies}
\end{figure}
\begin{table}[H]
	\centering
	\caption{Robotic Application Latency Statistics Tuned Virtualization}
	\label{tab:robotic_application_latency_values_virt_tun}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{Samples} & \textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Std Dev (µs)} \\ \hline
			1000             & 1.219                 & 2.62                  & 3.988                 & 0.182                 \\ \hline
		\end{tabular}}
\end{table}

\clearpage

\chapter{Results}\label{cha:results}
The goal was to virtualize the Salamander 4 OS and bring its real-time performance closer to that of the bare metal version and guarantee deterministic and reliable behavior. This chapter summarizes the results after the real-time performance tunings, both in terms of the \texttt{latency} program and the robotic application. Figure~\ref{fig:max_latency_combined_results} displays the change in latency after each real-time performance tuning, detailed in Section~\ref{sec:real-time_tunings}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.70\columnwidth]{img/results/gnuplot_combined_max_latency_all.png}
	\caption[Comparison of Latency Distribution of Salamander 4 Configurations]{Comparison of Latency Distribution of Salamander 4 after Configurations}
	\label{fig:max_latency_combined_results}
\end{figure}

\noindent As mentioned earlier in Section~\ref{sec:approach}, these modifications were added sequentially and tested together. Previous tunings were not reverted when moving to the next tunings. Table~\ref{tab:latency_tables_combined} gives an overview of the most important metrics of the measurements after each tuning.

\begin{table}[H]
	\centering
	\small
	\caption{Comparison of Latency Statistics in Salamander 4 after Configurations}
	\label{tab:latency_tables_combined}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{Salamander 4 Version}      & \textbf{Lat Min (µs)} & \textbf{Lat Avg (µs)} & \textbf{Lat Max (µs)} & \textbf{Overruns} \\ \hline
		Salamander 4 Bare Metal      & 0.613                 & 1.380                 & 10.709                & 0                 \\ \hline
		Untuned Salamander 4 Virtualization & 2.536                 & 8.940                 & 707.622               & 43                \\ \hline
		After BIOS Configurations     & 0.969                 & 3.948                 & 457.545               & 22                \\ \hline
		After Kernel Configurations   & 2.545                 & 4.811                 & 21.694                & 0                 \\ \hline
		After Host Configurations     & 2.591                 & 4.834                 & 18.441                & 0                 \\ \hline
		After QEMU Configurations     & 2.614                 & 4.779                 & 17.134                & 0                 \\ \hline
	\end{tabular}
\end{table}

\clearpage

\noindent The improved latency was also tested with the robotic application in Subsection~\ref{sec:robotic_application}. The difference between the command time and the time the signal reaches the PWM module was measured 1,000 times to get reliable results for the bare metal, untuned, and tuned versions of Salamander 4. The results are visualized in Figure~\ref{fig:combined_latencies} and detailed in Table~\ref{tab:robotic_application_latency_values_combined}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/results/combined_latencies.png}
	\caption{Comparison of Robotic Application Latency after Configurations}
	\label{fig:combined_latencies}
\end{figure}

\begin{table}[H]
	\small
	\centering
	\caption{Comparison of Robotic Application Latency Statistics}
	\label{tab:robotic_application_latency_values_combined}
	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Salamander 4 Version}        & \textbf{Lat Min (ms)} & \textbf{Lat Avg (ms)} & \textbf{Lat Max (ms)} & \textbf{Std Dev (ms)} \\ \hline
			Salamander 4 Bare Metal              & 1.211                 & 1.347                 & 1.49                  & 0.082                 \\ \hline
			Untuned Salamander 4 Virtualization & 3.1                   & 24.603                & 129.46                & 13.876                \\ \hline
			Tuned Salamander 4 Virtualization   & 1.219                 & 2.62                  & 3.988                 & 0.812                 \\ \hline
		\end{tabular}}
\end{table}

\clearpage

\chapter{Discussion}\label{cha:discussion}
This master's thesis shows that the PREEMPT-RT patch along with the applied real-time performance tunings have a positive impact on reducing latency and improving determinism. The maximum latency in the final tuned version of Salamander 4 virtualization is significantly lower than in the untuned setup. The worst latency value in a 10-minute measurement span decreased from 707.622~µs to 17.134~µs after tuning BIOS, kernel, host, QEMU/KVM, and guest configurations. Especially the BIOS and kernel configurations played a crucial role in reducing the maximum latency, with a major fall to 21.694~µs. From this moment on, there were also no overruns in the test any longer. The host configurations, including CPU and interrupt affinity, and real-time prioritization of QEMU, further reduced latency down to 18.441~µs. The guest OS, Salamander 4, already had real-time capabilities through Xenomai. The focus was primarily on optimizing the host and virtualization layer to achieve the desired real-time performance. Finally, QEMU/KVM configurations, such as tuning the LAPIC timer advance and using hugepages, brought the latency down to the final worst latency value of 17.134~µs in 10 minutes. This value is very close to the bare metal value of 10.709~µs. The goal was set to achieving latency values below 50 µs in the duration of the measurement. This goal is achieved.

\bigskip \noindent As a next step, the robotic application in a practical scenario also demonstrates and validates the improvement of the tuned virtualization. The difference between the command time and the time the signal reaches the PWM module was measured 1,000 times for the bare metal, untuned, and tuned versions of Salamander 4. The most important metric here is also the worst latency of the signal reaching the PWM. It dropped from 129~ms to 3.988~ms in the developed application. Comparing this to the worst latency value of the bare metal version, which is 1.49~ms, it is apparent that the tuned virtualization through real-time performance configurations came very close to the determinism and reliability of the hardware.

\bigskip \noindent It is important to underline one more time the fact that these modifications were added sequentially and tested together. After applying the PREEMPT-RT patch, the BIOS was configured, followed by the kernel. The BIOS settings were not reverted when moving to the kernel configurations. Next, the host was configured, but the BIOS and kernel settings remained unchanged. The guest configurations were not changed since Xenomai already provides real-time capabilities. Finally, QEMU/KVM settings were applied. This means that the configurations are not isolated but are applied and then tested as a whole.

\clearpage

\chapter{Summary and Outlook}\label{cha:summary_and_outlook}
Hardware-based systems are limited in adaptability and can be costly, especially when scaling operations. Physical access for updates and maintenance is challenging, leading to downtime and lost productivity. While virtualization addresses these issues, it introduces increased overhead and latency.

\bigskip \noindent In this master's thesis, the goal was the virtualization of a real-time operating system to control a robot, with a focus on compliance with real-time determinism. Specifically, the aim was to bring the latency of the virtualized Salamander 4 closer to that of the bare metal version, thereby providing a comprehensive blueprint for making a virtualized guest system in a host system real-time capable with deterministic behavior. Salamander 4 is built with Yocto, employs hard real-time with Xenomai 3, and is virtualized through QEMU/KVM. The testing robot was connected to the OS via a VARAN bus interface.

\bigskip \noindent The initial latency values were first measured with the \texttt{latency} program of the Xenomai tool suite. The tests were conducted for 10 minutes with a sampling period of 100 µs, using a periodic user-mode task, and were assigned a priority of 99. If any sample's latency value was greater than 100~µs, this was considered an overrun. There was a significant initial gap in latency statistics between the virtualized Salamander 4 and the Salamander 4 on bare metal. To address this gap, an extensive tuning process was carried out to achieve real-time performance and determinism. These modifications were added sequentially and tested together. After applying the PREEMPT-RT patch, the BIOS was configured, followed by the kernel. The BIOS settings were not reverted when moving to the kernel configurations. After these two steps, the worst latency decreased from an initial value of 707.62~µs to 21.694~µs. From this moment on, there were also no overruns in the tests any longer. The host configurations, including CPU and interrupt affinity, and real-time prioritization of QEMU, further reduced latency down to 18.441~µs. The guest OS, Salamander 4, already had real-time capabilities through Xenomai. The focus was primarily on optimizing the host and virtualization layer to achieve the desired real-time performance. Finally, QEMU/KVM configurations, such as tuning the LAPIC timer advance and using hugepages, brought the latency down a little more to the final worst latency value of 17.134~µs in 10 minutes. This value is very close to the bare metal value of 10.709~µs. The goal was set to achieving latency values below 50 µs in the duration of the measurement. This goal was achieved.

\bigskip \noindent On top of that, a robotic application further confirmed the improvements. The difference between the command time and the time the signal reaches the PWM module was measured 1,000 times for the bare metal, untuned, and tuned versions of Salamander 4. The worst latency of the signal reaching the PWM dropped from an initial value of 129~ms to 3.988~ms. Comparing this to the worst latency value of the bare metal version, which was 1.49~ms, it is apparent that the tuned virtualization through real-time performance configurations came very close to the determinism and reliability of the hardware.

\bigskip \noindent While this thesis provides a comprehensive blueprint for making a virtualized guest system in a host system real-time capable with deterministic behavior, future work can be done to extend this knowledge. Additional configurations and optimizations of the virtualization layer can be done and tested to further reduce latency and improve determinism. Other than that, the use of other hypervisors and virtualization technologies can be investigated to compare their performance for real-time applications. Moreover, extensive testing of the virtualized system under various workloads can be conducted to evaluate its performance and reliability in different conditions and stressed situations, as demonstrated by \citeauthor{huang2015performance}~\cite{huang2015performance}, \citeauthor{kirovaImpactModernVirtualization2019}~\cite{kirovaImpactModernVirtualization2019}, or \citeauthor{adamPerformanceAssessmentLinux2021}~\cite{adamPerformanceAssessmentLinux2021}.

%
% Hier beginnen die Verzeichnisse.
%

\clearpage

\printbibliography

\clearpage

% Das Abbildungsverzeichnis
\listoffigures

\clearpage

% Das Tabellenverzeichnis
\listoftables

\clearpage

% Das Quellcodeverzeichnis
\listofcode

\clearpage

\phantomsection
\addcontentsline{toc}{chapter}{\listacroname}

\chapter*{\listacroname}
\begin{acronym}[XXXXX]
	\acro{AI}[AI]{Artificial Intelligence}
	\acro{CID}[CID]{Context Identifier}
	\acro{CPU}[CPU]{Central Processing Unit}
	\acro{GPOS}[GPOS]{General-Purpose Operating System}
	\acro{GUI}[GUI]{Graphical User Interface}
	\acro{IRQ}[IRQ]{Interrupt Request}
	\acro{IRT}[IRT]{Interrupt Response Time}
	\acro{ISR}[ISR]{Interrupt Service Routine}
	\acro{KVM}[KVM]{Kernel-based Virtual Machine}
	\acro{LAPIC}[LAPIC]{Local Advanced Programmable Interrupt Controller}
	\acro{LSB}[LSB]{Least Significant Bit}
	\acro{ML}[ML]{Machine Learning}
	\acro{MSB}[MSB]{Most Significant Bit}
	\acro{PLC}[PLC]{Programmable Logic Controller}
	\acro{PWM}[PWM]{Pulse Width Modulation}
	\acro{QEMU}[QEMU]{Quick Emulator}
	\acro{RTOS}[RTOS]{Real-Time Operating System}
	\acro{SMP}[SMP]{Symmetric Multiprocessing}
	\acro{vCPU}[vCPU]{Virtual Central Processing Unit}
	\acro{VM}[VM]{Virtual Machine}
	\acro{VMCS}[VMCS]{Virtual Machine Control Structure}
	\acro{vsock}[vsock]{Virtual Socket}
\end{acronym}
\end{document}